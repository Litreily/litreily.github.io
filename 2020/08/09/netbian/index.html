<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="simple life"><meta name="theme-color" content="#2d4356"><meta name="baidu-site-verification" content="pte8o83UGG"><title>Python网络爬虫8 - 爬取彼岸图网美图 | LITREILY</title><link rel="stylesheet" type="text/css" href="/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.png"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script><script>var _hmt = _hmt || [];
(function() {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?d55250b3059d32736607d30baa6e0ca2";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
})();</script></head><link rel="stylesheet" type="text/css" href="/plugins/prettify/doxy.css"><script type="text/javascript" src="/js/ready.js" async></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><body class="night"><div class="mobile-head" id="mobile-head"><div class="navbar-icon"><span></span><span></span><span></span></div><div class="navbar-title"><a href="/">LITREILY</a></div><div class="navbar-search"><!--= show a circle here--></div></div><div class="h-wrapper" id="menu"><nav class="h-head box"><div class="m-hdimg"><a class="hdimg img" href="/"><img class="nofancybox" src="/img/profile.jpg" width="128" height="128"></a><h1 class="ttl"><a href="/">LITREILY</a></h1></div><p class="m-desc">心之所向，无惧无悔,<br>愿求仁得仁，复无怨怼！</p><div class="m-nav"><ul><li><span class="dot">●</span><a href="/archives/">归档</a></li><li><span class="dot">●</span><a href="/categories/">分类</a></li><li><span class="dot">●</span><a href="/tags/">标签</a></li><li><span class="dot">●</span><a href="/about/">关于</a></li><li><span class="dot">●</span><a href="/notes/">笔记</a></li><li><span class="dot">●</span><a href="/atom.xml">RSS</a></li><li class="m-sch"><form class="form" id="j-formsch" method="get"><input class="txt" type="text" id="local-search-input" name="q" value="搜索" onfocus="if(this.value=='搜索'){this.value='';}" onblur="if(this.value==''){this.value='搜索';}"><input type="text" style="display:none;"></form></li></ul><div id="local-search-result"></div></div></nav></div><div id="back2Top"><a class="fa fa-arrow-up" title="Back to top" href="#"></a></div><div class="box" id="container"><div class="l-wrapper"><div class="l-content box"><div class="l-post l-post-art"><article class="p-art"><div class="p-header box"><h1 class="p-title">Python网络爬虫8 - 爬取彼岸图网美图</h1><div class="p-info"><span class="p-date"><i class="fa fa-calendar"></i><a href="/2020/08/09/netbian/">2020-08-09</a></span><span class="p-category"><i class="fa fa-folder"></i><a href="/categories/Python/">Python</a></span><span class="p-view" id="busuanzi_container_page_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_page_pv"></span></span></div></div><div class="p-content"><p><a href="http://pic.netbian.com/" target="_blank" rel="noopener">彼岸图网</a>收集了大量美图，是个不错的爬取对象。话不多说，直接上图。</p>
<p><img src="/assets/netbian/netbian.png" alt="彼岸图网"></p>
<h2 id="分析站点"><a href="#分析站点" class="headerlink" title="分析站点"></a>分析站点</h2><h3 id="分类列表"><a href="#分类列表" class="headerlink" title="分类列表"></a>分类列表</h3><p>爬取之前，自然要分析一波，这个站点的框架比较简单，从分类着手，共包含12个分类项。</p>
<ol>
<li>4K人物</li>
<li>4K动漫</li>
<li>4K动物</li>
<li>4K宗教</li>
<li>4K影视</li>
<li>4K明星</li>
<li>4K汽车</li>
<li>4K游戏</li>
<li>4K美女</li>
<li>4K美食</li>
<li>4K背景</li>
<li>4K风景</li>
</ol>
<p>名称都含有4K，但是获取原图是需要会员的，所以我这里获取的不是原图，而是详细页展示的大图。首先要获取的当然是分类页面的网址，看下面的DOM.</p>
<p><img src="/assets/netbian/categories.png" alt="分类信息"></p>
<p>通过<code>xpath</code> <code>//div[contains(@class, &quot;classify&quot;)]/a</code> 可以得到分类链接信息，从而可以得到分类名称和网址。</p>
<h3 id="缩略图列表"><a href="#缩略图列表" class="headerlink" title="缩略图列表"></a>缩略图列表</h3><p>接下来以<code>4k影视</code>为例，解析每个分类页面，从分类页面可以看到图片的缩略图列表，点击缩略图就能进入详细页面看到大图。</p>
<p><img src="/assets/netbian/category.png" alt="4k影视"></p>
<p>缩略图列表中的图片链接可以通过<code>xpath</code> <code>//div[@class=&quot;slist&quot;]//a/@href</code> 获得。</p>
<p>此外，分类页面包含大量图片，是通过分页展示的，分页的页数可以从页面尾部看到。</p>
<p><img src="/assets/netbian/pages_num.png" alt="分类页面页码"></p>
<p>页面数量可以通过<code>xpath</code> <code>//span[@class=&quot;slh&quot;]/following-sibling::a[1]/text()</code>获得，也就是<code>...</code>后的同胞元素。</p>
<h3 id="大图页面"><a href="#大图页面" class="headerlink" title="大图页面"></a>大图页面</h3><p>最后就是通过缩略图访问的大图页面了，根据大图的<code>id</code>信息，其实际链接可以通过<code>xpath</code> <code>//*[@id=&quot;img&quot;]/img/@src</code>获得。</p>
<p><img src="/assets/netbian/image.png" alt="大图"></p>
<p>到此，整个网站已经分析完成。</p>
<h2 id="爬取方案"><a href="#爬取方案" class="headerlink" title="爬取方案"></a>爬取方案</h2><p>根据分析过程可以很容易想到爬取步骤：</p>
<ol>
<li>获取分类信息，包括名称和链接</li>
<li>根据分类链接爬取缩略图信息，逐页爬取</li>
<li>逐页爬取过程中，获取大图实际链接</li>
<li>下载大图到本地</li>
</ol>
<p>为了加速爬取过程，我们可以使用多进程，使用Python中的进程池<code>Pool</code>即可。</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>下面通过代码进行实现，为了方便资源共享，减少全局变量或参数传递，我将爬虫封装成一个类<code>Netbian_Spider</code>. 将主页网址和爬虫所需的<code>UA</code>放到初始化信息中。</p>
<pre><code class="python">class Netbian_Spider(object):
    def __init__(self):
        self.index = &#39;http://pic.netbian.com&#39;
        self.headers = {
            &#39;user-agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36&#39;
        }
</code></pre>
<h3 id="函数定义"><a href="#函数定义" class="headerlink" title="函数定义"></a>函数定义</h3><p>在类中，可以先按爬取步骤定义好成员函数，当然在编码过程中可以依情况进行增删改。</p>
<pre><code class="python">    def get_path(self, name):
        pass

    def get_categories(self):
        &#39;&#39;&#39;get categories of website&#39;&#39;&#39;
        pass

    def spider_by_category(self, category, url):
        &#39;&#39;&#39;Process function which use to capture images base on category&#39;&#39;&#39;
        pass

    def parse_thumb_page(self, url, first_page=False):
        &#39;&#39;&#39;parse thumbnail page and get all the detail pages url&#39;&#39;&#39;
        pass

    def parse_detail_page(self, url):
        &#39;&#39;&#39;parse detail page and get source image url&#39;&#39;&#39;
        pass

    def download_image(self, url, path):
        pass
</code></pre>
<p>下面对爬虫的实现过程进行详细说明。</p>
<h3 id="保存路径"><a href="#保存路径" class="headerlink" title="保存路径"></a>保存路径</h3><p>首先确定图片的保存路径，根目录为<code>~/Pictures/python/netbian</code>，<code>windows</code>对应用户默认的图片目录，<code>linux</code>用户也是同样。</p>
<p>调用<code>get_path</code>会在根目录下会根据分类名称<code>name</code>新建子文件夹。</p>
<pre><code class="python">    def get_path(self, name):
        home_path = os.path.expanduser(&#39;~&#39;)
        path = os.path.join(home_path, &#39;Pictures/python/netbian/&#39; + name)
        if not os.path.isdir(path):
            os.makedirs(path)

        return os.path.realpath(path)
</code></pre>
<h3 id="获取分类信息"><a href="#获取分类信息" class="headerlink" title="获取分类信息"></a>获取分类信息</h3><p>按照前面的分类，爬虫第一步是爬取分类信息，我们使用<code>yield</code>定义一个<strong>生成器</strong>，逐个返回获取到的分类名称和分类网址。</p>
<pre><code class="python">    def get_categories(self):
        &#39;&#39;&#39;get categories of website&#39;&#39;&#39;
        res = requests.get(self.index, headers=self.headers)
        doc = html.fromstring(res.content)
        categories = doc.xpath(&#39;//div[contains(@class, &quot;classify&quot;)]/a&#39;)

        for category in categories:
            name = category.xpath(&#39;text()&#39;)[0]
            url = category.xpath(&#39;@href&#39;)[0]
            yield name, url
</code></pre>
<h3 id="按分类逐页爬取"><a href="#按分类逐页爬取" class="headerlink" title="按分类逐页爬取"></a>按分类逐页爬取</h3><p>得到分类页面<code>url</code>后，通过后续实现的<code>page_thumb_page</code>解析分类页面得到</p>
<ol>
<li>大图详细页面链接<code>detail_pages</code></li>
<li>每个分类的总页面数量<code>page_cnt</code></li>
</ol>
<p>之后就逐页爬取大图并下载到本地，直到所有页面都爬取完成。</p>
<pre><code class="python">    def spider_by_category(self, category, url):
        &#39;&#39;&#39;Process function which use to capture images base on category&#39;&#39;&#39;
        path_category = self.get_path(category)
        detail_pages, page_cnt = self.parse_thumb_page(url, first_page=True)

        img_cnt = 0
        page_num = 1
        while True:
            for page in detail_pages:
                img_cnt += 1

                print(&#39;[{} page-{} img-{}] Parsing page {}&#39;.format(
                    category, page_num, img_cnt, page))
                img_url = self.parse_detail_page(page)
                self.download_image(img_url, path_category)

            page_num += 1
            if page_num &gt; page_cnt:
                break
            detail_pages = self.parse_thumb_page(
                &#39;{}index_{}.html&#39;.format(url, page_num))
</code></pre>
<h3 id="解析缩略图"><a href="#解析缩略图" class="headerlink" title="解析缩略图"></a>解析缩略图</h3><p>在分类页面，也就是缩略图页面，通过前面提及的<code>xpath</code>可以得到所有缩略图对应大图的链接。此外，如果是当前分类的首页，还需要返回分页数。</p>
<pre><code class="python">    def parse_thumb_page(self, url, first_page=False):
        &#39;&#39;&#39;parse thumbnail page and get all the detail pages url&#39;&#39;&#39;
        res = requests.get(self.index + url, headers=self.headers)
        doc = html.fromstring(res.content)
        detail_pages = doc.xpath(&#39;//div[@class=&quot;slist&quot;]//a/@href&#39;)

        if first_page:
            page_cnt = doc.xpath(
                &#39;//span[@class=&quot;slh&quot;]/following-sibling::a[1]/text()&#39;)[0]
            return detail_pages, int(page_cnt)
        else:
            return detail_pages
</code></pre>
<h3 id="下载大图"><a href="#下载大图" class="headerlink" title="下载大图"></a>下载大图</h3><p>大图页面的解析也是一个<code>xpath</code>就搞定了，然后通过<code>requests</code>下载到本地指定路径就ok啦。</p>
<pre><code class="python">    def parse_detail_page(self, url):
        &#39;&#39;&#39;parse detail page and get source image url&#39;&#39;&#39;
        res = requests.get(self.index + url, headers=self.headers)
        doc = html.fromstring(res.content)
        img_url = doc.xpath(&#39;//*[@id=&quot;img&quot;]/img/@src&#39;)[0]

        return img_url

    def download_image(self, url, path):
        img_name = url.split(&#39;/&#39;)[-1]
        save_path = os.path.join(path, img_name)

        res = requests.get(self.index + url, headers=self.headers, timeout=20)
        if res.status_code == 200:
            with open(save_path, &#39;wb&#39;) as f:
                f.write(res.content)
</code></pre>
<h3 id="main函数"><a href="#main函数" class="headerlink" title="main函数"></a>main函数</h3><p>主函数用到的最关键的知识点就是进程池<code>Pool</code>，使用Pool创建多进程，进程数量由<code>multiprocessing.cpu_conut()</code>决定，也就是PC包含的CPU数量。</p>
<p>主函数首先创建<code>Netbian_Spider</code>类的对象<code>spider</code>，然后获取分类信息。进程池中，每个进程处理一个分类，共12个进程，每次最多执行<code>cpu_count()</code>个进程，剩下的需要前面至少一个执行结束才会开始。</p>
<pre><code class="python">def main():
    spider = Netbian_Spider()
    categories = spider.get_categories()

    p = Pool(cpu_count())
    for name, url in categories:
        p.apply_async(spider.spider_by_category, args=(name, url))

    p.close()
    p.join()
    print(&#39;All Done!&#39;)


if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<h2 id="爬取测试"><a href="#爬取测试" class="headerlink" title="爬取测试"></a>爬取测试</h2><p>使用<code>python3</code>爬取彼岸图网，共爬取图片17796张，9.18G</p>
<p><img src="/assets/netbian/captureing.png" alt="爬取测试"></p>
<p><img src="/assets/netbian/captured.png" alt="爬取结果"></p>
<h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>该爬虫源码已放置Github项目<a href="https://github.com/Litreily/capturer" target="_blank" rel="noopener">capturer</a>，欢迎交流。</p>
<p>此外，爬取图片仅供学习，不得商用哦。</p>
</div><div class="p-copyright"><blockquote><div class="p-copyright-author"><span class="p-copyright-key">本文作者：</span><span class="p-copytight-value"><a href="mailto:litreily@163.com">litreily</a></span></div><div class="p-copyright-link"><span class="p-copyright-key">本文链接：</span><span class="p-copytight-value"><a href="/2020/08/09/netbian/">https://www.litreily.top/2020/08/09/netbian/</a></span></div><div class="p-copyright-note"><span class="p-copyright-key">版权声明：</span><span class="p-copytight-value">本博客所有文章除特殊声明外，均采用<a rel="nofollow" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/"> CC BY-NC 4.0 </a>许可协议。转载请注明出处 <a href="https://www.litreily.top">litreily的博客</a>！</span></div></blockquote></div></article><div class="p-info box"><span class="p-tags"><i class="fa fa-tags"></i><a href="/tags/spider/">spider</a><a href="/tags/xpath/">xpath</a><a href="/tags/pool/">pool</a></span></div><aside id="toc"><div class="toc-title">目录</div><nav><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#分析站点"><span class="toc-number">1.</span> <span class="toc-text">分析站点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#分类列表"><span class="toc-number">1.1.</span> <span class="toc-text">分类列表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#缩略图列表"><span class="toc-number">1.2.</span> <span class="toc-text">缩略图列表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#大图页面"><span class="toc-number">1.3.</span> <span class="toc-text">大图页面</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#爬取方案"><span class="toc-number">2.</span> <span class="toc-text">爬取方案</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#代码实现"><span class="toc-number">3.</span> <span class="toc-text">代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#函数定义"><span class="toc-number">3.1.</span> <span class="toc-text">函数定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#保存路径"><span class="toc-number">3.2.</span> <span class="toc-text">保存路径</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#获取分类信息"><span class="toc-number">3.3.</span> <span class="toc-text">获取分类信息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#按分类逐页爬取"><span class="toc-number">3.4.</span> <span class="toc-text">按分类逐页爬取</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#解析缩略图"><span class="toc-number">3.5.</span> <span class="toc-text">解析缩略图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#下载大图"><span class="toc-number">3.6.</span> <span class="toc-text">下载大图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#main函数"><span class="toc-number">3.7.</span> <span class="toc-text">main函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#爬取测试"><span class="toc-number">4.</span> <span class="toc-text">爬取测试</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#写在最后"><span class="toc-number">5.</span> <span class="toc-text">写在最后</span></a></li></ol></nav></aside></div><section class="p-ext"><div class="l-pager l-pager-dtl box"><a class="prev" href="/2020/08/13/python-pdf/">&lt; python之给pdf添加页码</a><a class="next" href="/2020/06/13/code-server/">VPS搭建在线VSCode Server &gt;</a></div><div id="valine-comment"><style type="text/css">.night .v[data-class=v] a { color: #0F9FB4 !important; }
.night .v[data-class=v] a:hover { color: #216C73 !important; }
.night .v[data-class=v] li { list-style: inherit; }
.night .v[data-class=v] .vwrap { border: 1px solid #223441; border-radius: 0; }
.night .v[data-class=v] .vwrap:hover { box-shadow: 0 0 6px 1px #223441; }
.night .v[data-class=v] .vbtn { border-radius: 0; background: none; }
.night .v[data-class=v] .vlist .vcard .vh { border-bottom-color: #293D4E; }
.night .v[data-class=v] .vwrap .vheader .vinput { border-bottom-color: #223441; }
.night .v[data-class=v] .vwrap .vheader .vinput:focus { border-bottom-color: #339EB4; }
.night .v[data-class=v] code, .night .v[data-class=v] pre,.night .v[data-class=v] .vlist .vcard .vhead .vsys { background: #203240 !important; }
.night .v[data-class=v] code, .night .v[data-class=v] pre { color: #F0F0F0; font-size: 95%; }
.v[data-class=v] .vcards .vcard .vh {border-bottom-color: #223441; }
.night .v[data-class=v] .vcards .vcard .vcontent.expand:before {background: linear-gradient(180deg,rgba(38,57,73,.4),rgba(38,57,73,.9));}
.night .v[data-class=v] .vcards .vcard .vcontent.expand:after {background: rgba(38,57,73,.9)}
</style><div id="vcomment"></div><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'1ecKy4yk4u1R7C4tScKbnyq9-gzGzoHsz',
  appKey:'uvA3xgqNW3q8TGR483lxXcpB',
  lang: 'zh-cn',
  placeholder:'ヾﾉ≧∀≦)o Come on, say something...',
  avatar:'identicon',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></section><footer><p>Copyright © 2016 - 2020 <a href="/." rel="nofollow">LITREILY</a> | <strong><a rel="nofollow" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a></strong><br><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span></span> <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span></span> | Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a>Theme with<a rel="nofollow" target="_blank" href="https://github.com/litreily/snark-hexo"> snark.</a></p></footer></div></div></div><script type="text/javascript" src="/plugins/prettify/prettify.js"></script><script type="text/javascript" src="/js/search.js"></script><script type="text/javascript" src="/js/top.js"></script><script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
    search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.1" async></script></body></html>