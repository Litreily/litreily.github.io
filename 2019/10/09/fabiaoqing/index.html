<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="simple life"><meta name="theme-color" content="#2d4356"><meta name="baidu-site-verification" content="pte8o83UGG"><title>Python网络爬虫7 - 爬取表情包 | LITREILY</title><link rel="stylesheet" type="text/css" href="/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.png"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script><script>var _hmt = _hmt || [];
(function() {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?d55250b3059d32736607d30baa6e0ca2";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
})();</script></head><link rel="stylesheet" type="text/css" href="/plugins/prettify/doxy.css"><script type="text/javascript" src="/js/ready.js" async></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><body class="night"><div class="mobile-head" id="mobile-head"><div class="navbar-icon"><span></span><span></span><span></span></div><div class="navbar-title"><a href="/">LITREILY</a></div><div class="navbar-search"><!--= show a circle here--></div></div><div class="h-wrapper" id="menu"><nav class="h-head box"><div class="m-hdimg"><a class="hdimg img" href="/"><img class="nofancybox" src="/img/profile.jpg" width="128" height="128"></a><h1 class="ttl"><a href="/">LITREILY</a></h1></div><p class="m-desc">心之所向，无惧无悔,<br>愿求仁得仁，复无怨怼！</p><div class="m-nav"><ul><li><span class="dot">●</span><a href="/archives/">归档</a></li><li><span class="dot">●</span><a href="/categories/">分类</a></li><li><span class="dot">●</span><a href="/tags/">标签</a></li><li><span class="dot">●</span><a href="/about/">关于</a></li><li><span class="dot">●</span><a href="/notes/">笔记</a></li><li><span class="dot">●</span><a href="/atom.xml">RSS</a></li><li class="m-sch"><form class="form" id="j-formsch" method="get"><input class="txt" type="text" id="local-search-input" name="q" value="搜索" onfocus="if(this.value=='搜索'){this.value='';}" onblur="if(this.value==''){this.value='搜索';}"><input type="text" style="display:none;"></form></li></ul><div id="local-search-result"></div></div></nav></div><div id="back2Top"><a class="fa fa-arrow-up" title="Back to top" href="#"></a></div><div class="box" id="container"><div class="l-wrapper"><div class="l-content box"><div class="l-post l-post-art"><article class="p-art"><div class="p-header box"><h1 class="p-title">Python网络爬虫7 - 爬取表情包</h1><div class="p-info"><span class="p-date"><i class="fa fa-calendar"></i><a href="/2019/10/09/fabiaoqing/">2019-10-09</a></span><span class="p-category"><i class="fa fa-folder"></i><a href="/categories/Python/">Python</a></span><span class="p-view" id="busuanzi_container_page_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_page_pv"></span></span></div></div><div class="p-content"><p>为了逗女朋友开心，想找一堆表情包，那么作为一名程序员，自然是会想到用程序来完成这个事情，而Python爬虫就是一个非常好的方法。</p>
<p>我先找到了一个专门发布表情包的网站，就叫做<a href="https://www.fabiaoqing.com/" target="_blank" rel="noopener">发表情</a>网，可以通过搜索关键词得到大量相关的表情包，下面对这个网站的爬取进行详细介绍。</p>
<p><img src="/assets/emotion/fabiaoqing.png" alt="fabiaoqing"></p>
<h2 id="分析站点"><a href="#分析站点" class="headerlink" title="分析站点"></a>分析站点</h2><p>为了不引起不适，我选择搜索“你好”，出来的结果是这样的</p>
<p><img src="/assets/emotion/hello.png" alt="hello"></p>
<p>可以看到共有<code>688</code>个相关的<strong>表情</strong>，右侧的<strong>表情包</strong>是分组形式的，不是我的爬取对象。我们只关注表情，在网页底部可以看到分页信息：</p>
<p><img src="/assets/emotion/page.png" alt="page"></p>
<p>切换分页到第４页后，可以看到网址的编号情况如下：</p>
<p><img src="/assets/emotion/hello-2.png" alt="hello page 2"></p>
<p>网址是<a href="https://www.fabiaoqing.com/search/search/keyword/你好/type/bq/page/4.html" target="_blank" rel="noopener">https://www.fabiaoqing.com/search/search/keyword/你好/type/bq/page/4.html</a>，据此就可以分析出表情的分页格式</p>
<ol>
<li><code>keyword</code> 后接关键词，此处为“你好”</li>
<li><code>type</code> 后接类型，此处为&quot;bq&quot;,对应<strong>表情</strong></li>
<li><code>page</code> 后接页码，此处为&quot;4.html&quot;，对应第４页</li>
</ol>
<p>到此就确定了关键词和分页信息的整合方式，根据这个可以按顺序获取指定关键词的所有分页。那么接下来的问题是如何解析每个分页中的图片信息，下面请看：</p>
<p><img src="/assets/emotion/hello-3.png" alt="hello page 3"></p>
<p>每个图片都在一个<code>div</code>标签内，使用<code>chrome</code>的调试工具可以获取到图片元素的<code>xpath</code>路径，然后稍作修改就可以匹配到当前页的所有图片</p>
<pre><code class="python">imgs = page.xpath(&#39;//div[@class=&quot;searchbqppdiv tagbqppdiv&quot;]//img/@data-original&#39;)
</code></pre>
<p>好了，到此，网站的分页已经每页的图片获取方式都知道了，下面开始完成代码的编写↓</p>
<h2 id="爬虫的代码实现"><a href="#爬虫的代码实现" class="headerlink" title="爬虫的代码实现"></a>爬虫的代码实现</h2><ol>
<li>获取指定关键词的所有分页</li>
<li>从分页中获取所有表情图片的链接</li>
<li>下载图片并存储到本地</li>
</ol>
<h3 id="get-imgs"><a href="#get-imgs" class="headerlink" title="get_imgs"></a>get_imgs</h3><p>首先对单个关键词，实现分页和图片链接的获取</p>
<pre><code class="python">def get_imgs(keyword):
    &#39;&#39;&#39;爬取某一个关键词相关的所有表情包
    Args:
        keyword: 表情包关键词
    &#39;&#39;&#39;
    page_index = 0
    img_cnts = 0
    save_dir = get_path(keyword)
    while True:
        page_index = page_index + 1
        # https://fabiaoqing.com/search/search/keyword/抱抱/type/bq/page/1.html
        url = &#39;{}{}/type/bq/page/{}.html&#39;.format(base_url, keyword, page_index)
        response = requests.get(url, headers=headers).content
        page = html.fromstring(response)
        imgs = page.xpath(
            &#39;//div[@class=&quot;searchbqppdiv tagbqppdiv&quot;]//img/@data-original&#39;)

        print(&#39;爬取 &quot;{}&quot; 相关表情包第 {} 页:&#39;.format(keyword, page_index))
        img_cnts = download_imgs(imgs, img_cnts, save_dir)

        if page_index == 1:
            page_num = int(
                page.xpath(&#39;//*[@id=&quot;mobilepage&quot;]/text()&#39;)[0].split()[-1])

        if page_index == page_num:
            break

    return img_cnts, save_dir
</code></pre>
<p>思路很简单，就是从１不断增加分页页码<code>page_index</code>，然后使用<code>lxml</code>中的<code>xpath</code>解析网页得到图片链接，最后使用下面介绍的<code>download_imgs</code>函数下载图片。那么什么时候停止获取呢？就是比较首页获取的总分页数<code>page_num</code>和当前分页数<code>page_index</code>是否一致，如果一致说明已经到最后一页。</p>
<p>函数中的<code>get_path</code>是个获取本地存储链接的函数，<code>base_url</code>和<code>headers</code>是两个全局变量</p>
<pre><code class="python">base_url = &#39;https://fabiaoqing.com/search/search/keyword/&#39;
headers = {
    &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.75 Safari/537.36&#39;
}


def get_path(keyword):
    &#39;&#39;&#39;生成指定关键词对应的表情包存储路径&#39;&#39;&#39;
    home_path = os.path.expanduser(&#39;~&#39;)
    path = os.path.join(home_path, &#39;Pictures/python/表情包/&#39; + keyword)
    if not os.path.isdir(path):
        os.makedirs(path)

    return os.path.realpath(path)
</code></pre>
<h3 id="download-imgs"><a href="#download-imgs" class="headerlink" title="download_imgs"></a>download_imgs</h3><pre><code class="python">def download_imgs(img_urls, starti, save_dir):
    &#39;&#39;&#39;下载单个页面内所有图片
    Args:
        img_urls: 关键词相关表情包某一分页的所有图片链接
        starti: 当前页面首个图片命名id
        save_dir: 图片存储路径
    &#39;&#39;&#39;
    fid = starti
    for img in img_urls:
        print(&#39;\t&#39; + img)
        fid = fid + 1
        file_name = &#39;{}.{}&#39;.format(fid, os.path.basename(img).split(&#39;.&#39;)[-1])
        save_path = os.path.join(save_dir, file_name)

        try:
            with open(save_path, &#39;wb&#39;) as f:
                f.write(requests.get(img, headers=headers, timeout=20).content)
        except requests.exceptions.ConnectionError as ce:
            print(ce.strerror())
        except requests.exceptions.MissingSchema:
            print(img + &#39; missing schema&#39;)
        except requests.exceptions.ReadTimeout:
            print(&#39;get {} timeout, skip this item.&#39;.format(img))
        finally:
            pass

    return fid
</code></pre>
<p>别看代码这么长，下载图片的关键就两行，和以往下载图片的方式大同小异，不再赘述。</p>
<pre><code class="python">with open(save_path, &#39;wb&#39;) as f:
    f.write(requests.get(img, headers=headers, timeout=20).content)
</code></pre>
<h3 id="main"><a href="#main" class="headerlink" title="main"></a>main</h3><p>主函数也比较简单，从命令行参数中获取所有关键词，逐个对关键词进行对应的表情爬取并统计爬取到的表情数。</p>
<pre><code class="python">def main():
    if len(sys.argv) &lt; 2:
        usage()
        sys.exit(0)

    print(&#39;============================================&#39;)
    for keyword in sys.argv[1:]:
        print(&#39;开始爬取关键词为 &quot;{}&quot; 的表情包:&#39;.format(keyword))
        count, save_dir = get_imgs(keyword)
        print(&#39;共爬取 &quot;{}&quot; 表情包 {} 个&#39;.format(keyword, count))
        print(&#39;文件存储于&quot;{}&quot;&#39;.format(save_dir))
    print(&#39;\n爬取完成！&#39;)
    print(&#39;============================================&#39;)
</code></pre>
<p>由于我们的目的是根据输入的关键词参数获取相关的表情，代码完成后的使用方式是这样的：</p>
<pre><code class="bash">python fabiaoqing_spider.py keyword1 keyword2 ...
</code></pre>
<p>所以<code>usage</code>函数是这样的，可以同时爬取多个关键词</p>
<pre><code class="python">def usage():
    print(&#39;Usage:\n\t&#39; + os.path.basename(sys.argv[0]) +
          &#39; [key1] [key2] [key3] ...\n&#39;)
</code></pre>
<h2 id="爬取测试"><a href="#爬取测试" class="headerlink" title="爬取测试"></a>爬取测试</h2><pre><code class="bash">python fabiaoqing_spider.py 你好 抱抱 亲亲
</code></pre>
<p><img src="/assets/emotion/test1.png" alt="test1"></p>
<p><img src="/assets/emotion/test2.png" alt="test2"></p>
<p><img src="/assets/emotion/test3.png" alt="test3"></p>
<p>上面是截取的部分爬取过程，下面是<strong>亲亲</strong>表情包的爬取结果</p>
<p><img src="/assets/emotion/test4.png" alt="test4"></p>
<p>代码已托管至<a href="https://github.com/Litreily/capturer" target="_blank" rel="noopener">Github capturer</a>项目，欢迎交流。</p>
<p>好啦，最后祝大家撩妹成功！</p>
</div><div class="p-copyright"><blockquote><div class="p-copyright-author"><span class="p-copyright-key">本文作者：</span><span class="p-copytight-value"><a href="mailto:litreily@163.com">litreily</a></span></div><div class="p-copyright-link"><span class="p-copyright-key">本文链接：</span><span class="p-copytight-value"><a href="/2019/10/09/fabiaoqing/">https://www.litreily.top/2019/10/09/fabiaoqing/</a></span></div><div class="p-copyright-note"><span class="p-copyright-key">版权声明：</span><span class="p-copytight-value">本博客所有文章除特殊声明外，均采用<a rel="nofollow" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/"> CC BY-NC 4.0 </a>许可协议。转载请注明出处 <a href="https://www.litreily.top">litreily的博客</a>！</span></div></blockquote></div></article><div class="p-info box"><span class="p-tags"><i class="fa fa-tag"></i><a href="/tags/spider/">spider</a></span></div><aside id="toc"><div class="toc-title">目录</div><nav><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#分析站点"><span class="toc-number">1.</span> <span class="toc-text">分析站点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#爬虫的代码实现"><span class="toc-number">2.</span> <span class="toc-text">爬虫的代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#get-imgs"><span class="toc-number">2.1.</span> <span class="toc-text">get_imgs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#download-imgs"><span class="toc-number">2.2.</span> <span class="toc-text">download_imgs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#main"><span class="toc-number">2.3.</span> <span class="toc-text">main</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#爬取测试"><span class="toc-number">3.</span> <span class="toc-text">爬取测试</span></a></li></ol></nav></aside></div><section class="p-ext"><div class="l-pager l-pager-dtl box"><a class="prev" href="/2019/11/03/kernel_debug/">&lt; watchdog bite导致系统重启问题的调试</a><a class="next" href="/2019/09/12/minicom/">嵌入式设备在无法使用网络和USB的情况下如何进行文件传输 &gt;</a></div><div id="valine-comment"><style type="text/css">.night .v[data-class=v] a { color: #0F9FB4 !important; }
.night .v[data-class=v] a:hover { color: #216C73 !important; }
.night .v[data-class=v] li { list-style: inherit; }
.night .v[data-class=v] .vwrap { border: 1px solid #223441; border-radius: 0; }
.night .v[data-class=v] .vwrap:hover { box-shadow: 0 0 6px 1px #223441; }
.night .v[data-class=v] .vbtn { border-radius: 0; background: none; }
.night .v[data-class=v] .vlist .vcard .vh { border-bottom-color: #293D4E; }
.night .v[data-class=v] .vwrap .vheader .vinput { border-bottom-color: #223441; }
.night .v[data-class=v] .vwrap .vheader .vinput:focus { border-bottom-color: #339EB4; }
.night .v[data-class=v] code, .night .v[data-class=v] pre,.night .v[data-class=v] .vlist .vcard .vhead .vsys { background: #203240 !important; }
.night .v[data-class=v] code, .night .v[data-class=v] pre { color: #F0F0F0; font-size: 95%; }
.v[data-class=v] .vcards .vcard .vh {border-bottom-color: #223441; }
.night .v[data-class=v] .vcards .vcard .vcontent.expand:before {background: linear-gradient(180deg,rgba(38,57,73,.4),rgba(38,57,73,.9));}
.night .v[data-class=v] .vcards .vcard .vcontent.expand:after {background: rgba(38,57,73,.9)}
</style><div id="vcomment"></div><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'1ecKy4yk4u1R7C4tScKbnyq9-gzGzoHsz',
  appKey:'uvA3xgqNW3q8TGR483lxXcpB',
  lang: 'zh-cn',
  placeholder:'ヾﾉ≧∀≦)o Come on, say something...',
  avatar:'identicon',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></section><footer><p>Copyright © 2016 - 2020 <a href="/." rel="nofollow">LITREILY</a> | <strong><a rel="nofollow" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a></strong><br><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span></span> <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span></span> | Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a>Theme with<a rel="nofollow" target="_blank" href="https://github.com/litreily/snark-hexo"> snark.</a></p></footer></div></div></div><script type="text/javascript" src="/plugins/prettify/prettify.js"></script><script type="text/javascript" src="/js/search.js"></script><script type="text/javascript" src="/js/top.js"></script><script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
    search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.1" async></script></body></html>