<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="simple life"><meta name="theme-color" content="#2d4356"><meta name="baidu-site-verification" content="pte8o83UGG"><title>Python网络爬虫6 - Scrapy爬取vmgirls | LITREILY</title><link rel="stylesheet" type="text/css" href="/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.png"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script><script>var _hmt = _hmt || [];
(function() {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?d55250b3059d32736607d30baa6e0ca2";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
})();</script></head><link rel="stylesheet" type="text/css" href="/plugins/prettify/doxy.css"><script type="text/javascript" src="/js/ready.js" async></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><body class="night"><div class="mobile-head" id="mobile-head"><div class="navbar-icon"><span></span><span></span><span></span></div><div class="navbar-title"><a href="/">LITREILY</a></div><div class="navbar-search"><!--= show a circle here--></div></div><div class="h-wrapper" id="menu"><nav class="h-head box"><div class="m-hdimg"><a class="hdimg img" href="/"><img class="nofancybox" src="/img/profile.jpg" width="128" height="128"></a><h1 class="ttl"><a href="/">LITREILY</a></h1></div><p class="m-desc">心之所向，无惧无悔,<br>愿求仁得仁，复无怨怼！</p><div class="m-nav"><ul><li><span class="dot">●</span><a href="/archives/">归档</a></li><li><span class="dot">●</span><a href="/categories/">分类</a></li><li><span class="dot">●</span><a href="/tags/">标签</a></li><li><span class="dot">●</span><a href="/about/">关于</a></li><li><span class="dot">●</span><a href="/notes/">笔记</a></li><li><span class="dot">●</span><a href="/atom.xml">RSS</a></li><li class="m-sch"><form class="form" id="j-formsch" method="get"><input class="txt" type="text" id="local-search-input" name="q" value="搜索" onfocus="if(this.value=='搜索'){this.value='';}" onblur="if(this.value==''){this.value='搜索';}"><input type="text" style="display:none;"></form></li></ul><div id="local-search-result"></div></div></nav></div><div id="back2Top"><a class="fa fa-arrow-up" title="Back to top" href="#"></a></div><div class="box" id="container"><div class="l-wrapper"><div class="l-content box"><div class="l-post l-post-art"><article class="p-art"><div class="p-header box"><h1 class="p-title">Python网络爬虫6 - Scrapy爬取vmgirls</h1><div class="p-info"><span class="p-date"><i class="fa fa-calendar"></i><a href="/2019/08/09/vmgirls/">2019-08-09</a></span><span class="p-category"><i class="fa fa-folder"></i><a href="/categories/Python/">Python</a></span><span class="p-view" id="busuanzi_container_page_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_page_pv"></span></span></div></div><div class="p-content"><p>今天介绍一个妹子站点图片的爬取过程，站点<a href="https://www.vmgirls.com/" target="_blank" rel="noopener">唯美女生</a>。站点结构非常简单，单独用<code>requests</code>库或者<code>scrapy</code>框架都可以。本文介绍的是使用<code>scrapy</code>框架爬取。</p>
<p><img src="/assets/vmgirls/vmgirls.png" alt="vmgirls"></p>
<a id="more"></a>
<h2 id="分析vmgirls站点"><a href="#分析vmgirls站点" class="headerlink" title="分析vmgirls站点"></a>分析vmgirls站点</h2><p>站点做的非常清新唯美，结构简洁明了，主页的主体部分以卡片形式展示各个主题的缩略图和简要介绍，但主页并不适合直接爬取。</p>
<p><img src="/assets/vmgirls/home.png" alt="vmgirls home page"></p>
<p>不过幸好该站点已经提供了站点地图，对于这种相对简单的网站，使用现成的网站地图简直事半功倍。</p>
<p><img src="/assets/vmgirls/sitemap.png" alt="sitemap"></p>
<p>那么思路就很清楚了，首先通过站点地图获取所有主题页面的网址和标题，然后逐个爬取妹子页面，提取所有的图片<code>url</code>，然后下载到本地，每个主题页面的图片单独存放到一个文件夹中。</p>
<h3 id="站点地图"><a href="#站点地图" class="headerlink" title="站点地图"></a>站点地图</h3><p>本站点提供了两个站点地图：</p>
<ol>
<li><a href="https://www.vmgirls.com/sitemap.shtml" target="_blank" rel="noopener">https://www.vmgirls.com/sitemap.shtml</a></li>
<li><a href="https://www.vmgirls.com/sitemap.xml" target="_blank" rel="noopener">https://www.vmgirls.com/sitemap.xml</a></li>
</ol>
<p>第1个是面向用户的站点地图，从主页的导航栏就能找到；第2个（如下图）则是常规网站留给搜索引擎的站点地图，文件格式也是常规的<code>xml</code></p>
<p><img src="/assets/vmgirls/sitemap_xml.png" alt="sitemap.xml"></p>
<p>我使用<code>wget</code>指令获取两个文件，并进行了对比，发现<code>sitemap.xml</code>仅仅包含1000个网址，但是可视的站点地图包含1163个网址。说明留给搜索引擎的地图并不完整，而且相比较之下，<code>sitemap.shtml</code>所占的文件大小要比<code>sitemap.xml</code>还要小，这是因为<code>sitemap.xml</code>因其格式问题，带有大量重复信息。</p>
<pre><code class="bash">➜  tmp ls -lh
total 320K
-rw-r--r-- 1 litreily litreily 132K 8月  26 17:25 sitemap.shtml
-rw-r--r-- 1 litreily litreily 186K 8月  26 20:44 sitemap.xml
</code></pre>
<p>所以，不管是考虑完整性，还是文件大小，我们都有理由选择<code>sitemap.shtml</code>作为爬取的第一个网页。</p>
<p>下面来看看站点地图的html结构：</p>
<p><img src="/assets/vmgirls/sitemap_struct.png" alt="sitemap structure"></p>
<p>使用<code>wget</code>获取该网页，截取其中一段列表信息如下：</p>
<pre><code class="html">&lt;h2 style=&quot;text-align: center; margin-top: 20px&quot;&gt;唯美女生站点地图&lt;/h2&gt;
&lt;center&gt;&lt;/center&gt;
&lt;div id=&quot;nav&quot;&gt;
&lt;a href=&quot;https://www.vmgirls.com/&quot;&gt;&lt;strong&gt;唯美女生&lt;/strong&gt;&lt;/a&gt; » &lt;a href=&quot;https://www.vmgirls.com/sitemap.shtml&quot;&gt;站点地图&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&quot;content&quot;&gt;
&lt;h3&gt;最新文章&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.vmgirls.com/12530.html&quot; title=&quot;倦怠&quot; target=&quot;_blank&quot;&gt;倦怠&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.vmgirls.com/12517.html&quot; title=&quot;逆光&quot; target=&quot;_blank&quot;&gt;逆光&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.vmgirls.com/12507.html&quot; title=&quot;回眸&quot; target=&quot;_blank&quot;&gt;回眸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.vmgirls.com/12487.html&quot; title=&quot;早安，乖乖&quot; target=&quot;_blank&quot;&gt;早安，乖乖&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.vmgirls.com/12477.html&quot; title=&quot;你眼里有我&quot; target=&quot;_blank&quot;&gt;你眼里有我&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.vmgirls.com/12476.html&quot; title=&quot;夏日限定&quot; target=&quot;_blank&quot;&gt;夏日限定&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.vmgirls.com/12419.html&quot; title=&quot;你眼睛一闪一闪的&quot; target=&quot;_blank&quot;&gt;你眼睛一闪一闪的&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.vmgirls.com/12405.html&quot; title=&quot;夏天你的甜&quot; target=&quot;_blank&quot;&gt;夏天你的甜&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.vmgirls.com/12386.html&quot; title=&quot;无尽夏&quot; target=&quot;_blank&quot;&gt;无尽夏&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.vmgirls.com/12353.html&quot; title=&quot;Halcyon&quot; target=&quot;_blank&quot;&gt;Halcyon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.vmgirls.com/12333.html&quot; title=&quot;少女情怀&quot; target=&quot;_blank&quot;&gt;少女情怀&lt;/a&gt;&lt;/li&gt;
</code></pre>
<p>从上图可以看出，网页网址以<code>&lt;li&gt;</code>列表形式存储，网页解析时使用以下xpath表达式即可获取到所有的网址和标题。</p>
<pre><code class="python"># example:
# &lt;li&gt;&lt;a href=&quot;https://www.vmgirls.com/12419.html&quot; title=&quot;你眼睛一闪一闪的&quot; target=&quot;_blank&quot;&gt;你眼睛一闪一闪的&lt;/a&gt;&lt;/li&gt;
urls = response.xpath(&#39;//*[@id=&quot;content&quot;][1]/ul/li/a/@href&#39;).extract()
titles = response.xpath(&#39;//*[@id=&quot;content&quot;][1]/ul/li/a/text()&#39;).extract()
</code></pre>
<p>据此，我们便拿到了所有页面的网址和标题，下面针对单个主题页面进行解析。</p>
<h3 id="单个主题页面"><a href="#单个主题页面" class="headerlink" title="单个主题页面"></a>单个主题页面</h3><p><img src="/assets/vmgirls/theme.png" alt="single theme"></p>
<p>针对单个主题页面，使用<code>Chrome</code>调试工具可以看到图片链接的<code>DOM</code>结构，但是要注意的是，调试工具看到的和实际wget获取到的不太一样，我猜测是在浏览器上显示时执行了某些额外的JS脚本。不管怎么样，当然是要以爬取后的实际数据为准，如下信息所示，img标签的<code>src</code>属性与上图中的不一样，所以我们在解析时不能使用这个属性，但是<code>data-src</code>则是相同的。</p>
<pre><code class="html">&lt;div class=&quot;post-content&quot;&gt;
&lt;div class=&quot;nc-light-gallery&quot;&gt;
&lt;p&gt;◎摄影/后期：@萌琦琦M77&lt;br&gt; – – – – – – – –&lt;br&gt; 你的眼睛里好像有闪闪的星星坠落。&lt;br&gt;
&lt;a href=&quot;https://static.vmgirls.com/image/2019/07/2019-07-18_00-53-58.jpg&quot; alt=&quot;你眼睛一闪一闪的&quot; title=&quot;你眼睛一闪一闪的&quot;&gt;&lt;img alt=&quot;你眼睛一闪一闪的-唯美女生&quot; src=&quot;data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7&quot; data-src=&quot;https://static.vmgirls.com/image/2019/07/2019-07-18_00-53-58.jpg&quot; data-nclazyload=&quot;true&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://static.vmgirls.com/image/2019/07/2019-07-18_00-54-01.jpg&quot; alt=&quot;你眼睛一闪一闪的&quot; title=&quot;你眼睛一闪一闪的&quot;&gt;&lt;img alt=&quot;你眼睛一闪一闪的-唯美女生&quot; src=&quot;data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7&quot; data-src=&quot;https://static.vmgirls.com/image/2019/07/2019-07-18_00-54-01.jpg&quot; data-nclazyload=&quot;true&quot;&gt;&lt;/a&gt;
</code></pre>
<p>由上面的分析可以确定出以下解析方式，以获取到当前页面所有图片的下载链接。</p>
<pre><code class="python">urls = response.xpath(&#39;//*[@class=&quot;post-content&quot;]//img/@data-src&#39;).extract()
</code></pre>
<p>至此，单个页面的解析思路也清楚了，之后通过<code>Scrapy</code>的<code>ImagesPipeline</code>即可完成图片下载。</p>
<h2 id="爬虫的代码实现"><a href="#爬虫的代码实现" class="headerlink" title="爬虫的代码实现"></a>爬虫的代码实现</h2><h3 id="创建scrapy项目"><a href="#创建scrapy项目" class="headerlink" title="创建scrapy项目"></a>创建scrapy项目</h3><pre><code class="bash">$ scrapy startproject vmgirls
$ cd vmgirls
$ scrapy genspider vmgirl www.vmgirls.com
$ tree
.
├── scrapy.cfg
└── vmgirl
    ├── __init__.py
    ├── items.py
    ├── middlewares.py
    ├── pipelines.py
    ├── settings.py
    └── spiders
        ├── __init__.py
        └── vmgirl.py
</code></pre>
<h3 id="全局配置"><a href="#全局配置" class="headerlink" title="全局配置"></a>全局配置</h3><p>编辑settings.py</p>
<pre><code class="python">BOT_NAME = &#39;vmgirls&#39;

SPIDER_MODULES = [&#39;vmgirls.spiders&#39;]
NEWSPIDER_MODULE = &#39;vmgirls.spiders&#39;

import os
USER_DIR = os.path.expanduser(&#39;~&#39;)
USER_DATA_DIR = os.path.join(USER_DIR, &#39;Pictures/python/vmgirls&#39;)
IMAGES_STORE = USER_DATA_DIR

# Obey robots.txt rules
ROBOTSTXT_OBEY = False

# Configure item pipelines
# See https://doc.scrapy.org/en/latest/topics/item-pipeline.html
ITEM_PIPELINES = {
    &#39;vmgirls.pipelines.VmgirlsPipeline&#39;: 300,
    &#39;vmgirls.pipelines.VmgirlsImagesPipeline&#39;: 400
}
</code></pre>
<p>如上代码所示，全局配置文件主要定义了文件存储路径<code>IMAGES_STORE</code>, 禁止遵守robots.txt, 启用两个Pipeline并设置其优先级。</p>
<h3 id="定义Item"><a href="#定义Item" class="headerlink" title="定义Item"></a>定义Item</h3><p>接下来定义两个Item类，<code>VmgirlsItem</code>用于存储主题页面的网址及其标题，<code>VmgirlsImagesItem</code>用于存储单个主题页面内所有图片的地址和主题的标题。这两个类的title内容是一致的。</p>
<pre><code class="python">from scrapy.item import Item
from scrapy.item import Field


class VmgirlsItem(Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    url = Field()
    title = Field()
    pass

class VmgirlsImagesItem(Item):
    image_urls = Field()
    title = Field()
    pass
</code></pre>
<h3 id="编写爬虫"><a href="#编写爬虫" class="headerlink" title="编写爬虫"></a>编写爬虫</h3><p>爬虫当然是最关键的一步，爬取思路与开始提及的站点分析思路一致，先爬取站点地图，然后在解析函数<code>parse</code>中获取所有主题页面的网址和标题，并通过<code>VmgirlsItem</code>提交到项目管道；与此同时，将爬取到的主题页面提交给引擎，由引擎把需求转给调度器和下载器；这一步爬取的结果由新的解析函数<code>parse_page</code>处理。</p>
<p>解析函数<code>parse_page</code>会将每个主题页面的图片链接和标题提取出来，然后提交给项目管道，由pipeline部分完成图片下载操作。</p>
<pre><code class="python">import scrapy

from vmgirls.items import VmgirlsItem
from vmgirls.items import VmgirlsImagesItem

from scrapy.http import Request
from scrapy.utils.project import get_project_settings

import os

class VmgirlSpider(scrapy.Spider):
    name = &#39;vmgirl&#39;
    allowed_domains = [&#39;vmgirls.com&#39;]
    start_urls = [&#39;https://www.vmgirls.com/sitemap.shtml/&#39;]

    def __init__(self):
        settings = get_project_settings()
        self.user_data_dir = settings.get(&#39;USER_DATA_DIR&#39;)

    def parse(self, response):
        &#39;&#39;&#39;Parse sitemap&#39;&#39;&#39;
        urls = response.xpath(&#39;//*[@id=&quot;content&quot;][1]/ul/li/a/@href&#39;).extract()
        titles = response.xpath(&#39;//*[@id=&quot;content&quot;][1]/ul/li/a/text()&#39;).extract()

        item = VmgirlsItem()
        item[&#39;theme_urls&#39;] = urls
        item[&#39;theme_titles&#39;] = titles
        yield item

        for url, title in zip(urls, titles):
            save_path = os.path.join(self.user_data_dir, title)
            if not os.path.isdir(save_path):
                os.makedirs(save_path)

            yield Request(url, meta={&#39;title&#39;: title}, callback=self.parse_page)

    def parse_page(self, response):
        &#39;&#39;&#39;Parse each page of girls&#39;&#39;&#39;
        urls = response.xpath(&#39;//*[@class=&quot;post-content&quot;]//img/@data-src&#39;).extract()
        item = VmgirlsImagesItem()
        item[&#39;image_urls&#39;] = urls
        item[&#39;title&#39;] = response.meta[&#39;title&#39;]
        yield item
</code></pre>
<h3 id="编写Pipeline"><a href="#编写Pipeline" class="headerlink" title="编写Pipeline"></a>编写Pipeline</h3><p>爬虫爬取的信息最终通过项目管道进行持久化处理或者完成相应的资源下载任务，从之前的配置文件我们也能知道，我们需要两个<code>pipeline</code>类，一个处理<code>VmgirlsItem</code>,另一个处理<code>VmgirlsImagesPipeline</code>，下面逐一介绍。</p>
<pre><code class="python">from scrapy.exporters import JsonLinesItemExporter
from scrapy.pipelines.images import ImagesPipeline
from scrapy.exceptions import DropItem
from scrapy.http import Request

from vmgirls.items import VmgirlsItem
from vmgirls.items import VmgirlsImagesItem

import os


class VmgirlsPipeline(object):
    &#39;&#39;&#39;Pipeline for every url of one theme, save theme info to json file&#39;&#39;&#39;
    def __init__(self, user_data_dir):
        &#39;&#39;&#39;Open file to save the exported Items&#39;&#39;&#39;
        self.user_data_dir = user_data_dir

        if not os.path.isdir(self.user_data_dir):
            os.makedirs(self.user_data_dir)

    @classmethod
    def from_crawler(cls, crawler):
        &#39;&#39;&#39;Get user dir from global settings.py&#39;&#39;&#39;
        settings = crawler.settings
        return cls(settings.get(&#39;USER_DATA_DIR&#39;))

    def process_item(self, item, spider):
        &#39;&#39;&#39;Save item info to loacl file&#39;&#39;&#39;
        if isinstance(item, VmgirlsItem):
            self.girls_info = open(
                os.path.join(self.user_data_dir, &#39;vmgirls.json&#39;), &#39;w+b&#39;)
            self.girls_exporter = JsonLinesItemExporter(
                self.girls_info, encoding=&#39;utf-8&#39;, indent=4)

            self.girls_exporter.start_exporting()

            for url, title in zip(item[&#39;theme_urls&#39;], item[&#39;theme_titles&#39;]):
                single_item = {&#39;theme_url&#39;:url, &#39;title&#39;:title}
                self.girls_exporter.export_item(single_item)

            self.girls_exporter.finish_exporting()
            self.girls_info.close()
        return item
</code></pre>
<p>第一个<code>pipeline</code>类将站点地图的信息以<code>json</code>格式存储到文件<code>vmgirls.json</code>中。这里用到了<code>JsonLinesItemExporter</code>,该类可以将一个个<code>dict</code>数据以单行形式转化成json格式。</p>
<pre><code class="python">class VmgirlsImagesPipeline(ImagesPipeline):
    &#39;&#39;&#39;Get images from one theme&#39;&#39;&#39;
    def get_media_requests(self, item, info):
        if isinstance(item, VmgirlsImagesItem):
            for image_url in item[&#39;image_urls&#39;]:
                yield Request(image_url, meta={&#39;item&#39;: item})

    def file_path(self, request, response=None, info=None):
        &#39;&#39;&#39;Set image dir to IMAGES_STORE/title/base_url&#39;&#39;&#39;
        url = request.url
        item = request.meta[&#39;item&#39;]
        path = os.path.join(item[&#39;title&#39;], url.split(&#39;/&#39;)[-1])
        return path

    def item_completed(self, results, item, info):
        if isinstance(item, VmgirlsImagesItem):
            image_paths = [x[&#39;path&#39;] for ok, x in results if ok]

            if not image_paths:
                raise DropItem(&quot;Item contains no images&quot;)
            return item
</code></pre>
<p>第二个<code>pipeline</code>类继承于<code>ImagesPipeline</code>，这是一个专门用于图片下载的管道类，配置文件中的<code>IMAGES_STORE</code>正是用于指定该类下载图片后的存放路径。</p>
<p>我们这里重写了三个函数：</p>
<ol>
<li><code>get_media_requests</code> 获取某主题页面的所有图片链接，使用<code>Request</code>进行下载</li>
<li><code>file_path</code> 为了将不同主题的图片存储在不同的文件夹，需要修改存储路径</li>
<li><code>item_completed</code> 当图片下载完成后，进入该函数，如果没有下载到图片则提示该<code>Item</code>不包含图片</li>
</ol>
<p>可以注意到，两个管道类在处理<code>item</code>前都有通过<code>isinstance</code>函数判断当前的<code>pipeline</code>属于哪个类的实例，这是为了保证一个管道类只处理对应的管道数据。因为默认情况下，爬虫提交出来的所有<code>item</code>都会根据<code>pipeline</code>的优先级依次经过两个管道，但我们实际只需要每个<code>item</code>经过对应的管道即可，所以使用<code>isinstance</code>进行判断是非常有必要的，既可以提高处理效率，又可以避免过多无效log对调试过程产生的干扰。</p>
<h2 id="爬取结果"><a href="#爬取结果" class="headerlink" title="爬取结果"></a>爬取结果</h2><p>为了方便爬取，可以使用以下的代码段作为爬虫的入口。</p>
<pre><code class="python">from scrapy import cmdline
cmdline.execute(&#39;scrapy crawl vmgirl&#39;.split())
</code></pre>
<p>执行后可以看到爬虫到的图片数据如下：</p>
<p><img src="/assets/vmgirls/pictures.png" alt="capture result"></p>
<p>代码已托管至开源项目<a href="https://github.com/Litreily/capturer" target="_blank" rel="noopener">litreily/capturer</a>, 欢迎Star和交流。</p>
</div><div class="p-copyright"><blockquote><div class="p-copyright-author"><span class="p-copyright-key">本文作者：</span><span class="p-copytight-value"><a href="mailto:litreily@163.com">litreily</a></span></div><div class="p-copyright-link"><span class="p-copyright-key">本文链接：</span><span class="p-copytight-value"><a href="/2019/08/09/vmgirls/">https://www.litreily.top/2019/08/09/vmgirls/</a></span></div><div class="p-copyright-note"><span class="p-copyright-key">版权声明：</span><span class="p-copytight-value">本博客所有文章除特殊声明外，均采用<a rel="nofollow" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/"> CC BY-NC 4.0 </a>许可协议。转载请注明出处 <a href="https://www.litreily.top">litreily的博客</a>！</span></div></blockquote></div></article><div class="p-info box"><span class="p-tags"><i class="fa fa-tags"></i><a href="/tags/spider/">spider</a><a href="/tags/girls/">girls</a></span></div><aside id="toc"><div class="toc-title">目录</div><nav><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#分析vmgirls站点"><span class="toc-number">1.</span> <span class="toc-text">分析vmgirls站点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#站点地图"><span class="toc-number">1.1.</span> <span class="toc-text">站点地图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#单个主题页面"><span class="toc-number">1.2.</span> <span class="toc-text">单个主题页面</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#爬虫的代码实现"><span class="toc-number">2.</span> <span class="toc-text">爬虫的代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#创建scrapy项目"><span class="toc-number">2.1.</span> <span class="toc-text">创建scrapy项目</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#全局配置"><span class="toc-number">2.2.</span> <span class="toc-text">全局配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#定义Item"><span class="toc-number">2.3.</span> <span class="toc-text">定义Item</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#编写爬虫"><span class="toc-number">2.4.</span> <span class="toc-text">编写爬虫</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#编写Pipeline"><span class="toc-number">2.5.</span> <span class="toc-text">编写Pipeline</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#爬取结果"><span class="toc-number">3.</span> <span class="toc-text">爬取结果</span></a></li></ol></nav></aside></div><section class="p-ext"><div class="l-pager l-pager-dtl box"><a class="prev" href="/2019/09/02/lftp/">&lt; 使用正则模糊匹配的ftp文件传输</a><a class="next" href="/2019/06/11/mind-map/">高效思维导图应用训练 &gt;</a></div><div id="valine-comment"><style type="text/css">.night .v[data-class=v] a { color: #0F9FB4 !important; }
.night .v[data-class=v] a:hover { color: #216C73 !important; }
.night .v[data-class=v] li { list-style: inherit; }
.night .v[data-class=v] .vwrap { border: 1px solid #223441; border-radius: 0; }
.night .v[data-class=v] .vwrap:hover { box-shadow: 0 0 6px 1px #223441; }
.night .v[data-class=v] .vbtn { border-radius: 0; background: none; }
.night .v[data-class=v] .vlist .vcard .vh { border-bottom-color: #293D4E; }
.night .v[data-class=v] .vwrap .vheader .vinput { border-bottom-color: #223441; }
.night .v[data-class=v] .vwrap .vheader .vinput:focus { border-bottom-color: #339EB4; }
.night .v[data-class=v] code, .night .v[data-class=v] pre,.night .v[data-class=v] .vlist .vcard .vhead .vsys { background: #203240 !important; }
.night .v[data-class=v] code, .night .v[data-class=v] pre { color: #F0F0F0; font-size: 95%; }
.v[data-class=v] .vcards .vcard .vh {border-bottom-color: #223441; }
.night .v[data-class=v] .vcards .vcard .vcontent.expand:before {background: linear-gradient(180deg,rgba(38,57,73,.4),rgba(38,57,73,.9));}
.night .v[data-class=v] .vcards .vcard .vcontent.expand:after {background: rgba(38,57,73,.9)}
</style><div id="vcomment"></div><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'1ecKy4yk4u1R7C4tScKbnyq9-gzGzoHsz',
  appKey:'uvA3xgqNW3q8TGR483lxXcpB',
  lang: 'zh-cn',
  placeholder:'ヾﾉ≧∀≦)o Come on, say something...',
  avatar:'identicon',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></section><footer><p>Copyright © 2016 - 2020 <a href="/." rel="nofollow">LITREILY</a> | <strong><a rel="nofollow" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a></strong><br><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span></span> <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span></span> | Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a>Theme with<a rel="nofollow" target="_blank" href="https://github.com/litreily/snark-hexo"> snark.</a></p></footer></div></div></div><script type="text/javascript" src="/plugins/prettify/prettify.js"></script><script type="text/javascript" src="/js/search.js"></script><script type="text/javascript" src="/js/top.js"></script><script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
    search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.1" async></script></body></html>