<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="simple life"><meta name="theme-color" content="#2d4356"><meta name="baidu-site-verification" content="pte8o83UGG"><title>Python网络爬虫2 - 爬取新浪微博用户图片 | LITREILY</title><link rel="stylesheet" type="text/css" href="/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.png"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script><script>var _hmt = _hmt || [];
(function() {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?d55250b3059d32736607d30baa6e0ca2";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
})();</script></head><link rel="stylesheet" type="text/css" href="/plugins/prettify/doxy.css"><script type="text/javascript" src="/js/ready.js" async></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><body class="night"><div class="mobile-head" id="mobile-head"><div class="navbar-icon"><span></span><span></span><span></span></div><div class="navbar-title"><a href="/">LITREILY</a></div><div class="navbar-search"><!--= show a circle here--></div></div><div class="h-wrapper" id="menu"><nav class="h-head box"><div class="m-hdimg"><a class="hdimg img" href="/"><img class="nofancybox" src="/img/profile.jpg" width="128" height="128"></a><h1 class="ttl"><a href="/">LITREILY</a></h1></div><p class="m-desc">心之所向，无惧无悔,<br>愿求仁得仁，复无怨怼！</p><div class="m-nav"><ul><li><span class="dot">●</span><a href="/archives/">归档</a></li><li><span class="dot">●</span><a href="/categories/">分类</a></li><li><span class="dot">●</span><a href="/tags/">标签</a></li><li><span class="dot">●</span><a href="/about/">关于</a></li><li><span class="dot">●</span><a href="/notes/">笔记</a></li><li><span class="dot">●</span><a href="/atom.xml">RSS</a></li><li class="m-sch"><form class="form" id="j-formsch" method="get"><input class="txt" type="text" id="local-search-input" name="q" value="搜索" onfocus="if(this.value=='搜索'){this.value='';}" onblur="if(this.value==''){this.value='搜索';}"><input type="text" style="display:none;"></form></li></ul><div id="local-search-result"></div></div></nav></div><div id="back2Top"><a class="fa fa-arrow-up" title="Back to top" href="#"></a></div><div class="box" id="container"><div class="l-wrapper"><div class="l-content box"><div class="l-post l-post-art"><article class="p-art"><div class="p-header box"><h1 class="p-title">Python网络爬虫2 - 爬取新浪微博用户图片</h1><div class="p-info"><span class="p-date"><i class="fa fa-calendar"></i><a href="/2018/04/10/sina/">2018-04-10</a></span><span class="p-category"><i class="fa fa-folder"></i><a href="/categories/Python/">Python</a></span><span class="p-view" id="busuanzi_container_page_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_page_pv"></span></span></div></div><div class="p-content"><p>其实，新浪微博用户图片爬虫是我学习<code>python</code>以来写的第一个爬虫，只不过当时懒，后来爬完<code>Lofter</code>后觉得有必要总结一下，所以就有了第一篇爬虫博客。现在暂时闲下来了，准备把新浪的这个也补上。</p>
<p>言归正传，既然选择爬新浪微博，那当然是有需求的，这也是学习的主要动力之一，没错，就是美图。<code>sina</code>用户多数微博都是包含图片的，而且是组图居多，单个图片的较少。</p>
<p>为了避免侵权，本文以本人微博<a href="https://weibo.com/litreily" target="_blank" rel="noopener">litreily</a>为例说明整个爬取过程，虽然图片较少，质量较低，但爬取方案是绝对ok的，使用时只要换个用户ID就可以了。</p>
<h2 id="分析sina站点"><a href="#分析sina站点" class="headerlink" title="分析sina站点"></a>分析sina站点</h2><h3 id="获取用户ID"><a href="#获取用户ID" class="headerlink" title="获取用户ID"></a>获取用户ID</h3><p>在爬取前，我们需要知道的是每个用户都有一个用户名，而一个用户名又对应一个唯一的整型数字ID，类似于学生的学号，本人的是<code>2657006573</code>。至于怎么根据用户名去获取ID，有以下两种方法：</p>
<ol>
<li>进入待爬取用户主页，在浏览器网址栏中即可看到一串数据，那就是用户ID</li>
<li><code>Ctrl-U</code>查看待爬取用户的源码，搜索<code>&quot;uid</code>，注意是<strong>双引号</strong></li>
</ol>
<p>其实是可以在已知用户名的情况下通过爬虫自动获取到<code>uid</code>的，但是我当时初学<code>python</code>，并没有考虑充分，所以后面的源码是以用户ID作为输入参数的。</p>
<h3 id="图片存储参数解析"><a href="#图片存储参数解析" class="headerlink" title="图片存储参数解析"></a>图片存储参数解析</h3><p>用户所有的图片都被存放至这样的路径下，真的是<strong>所有图片</strong>哦！！！</p>
<pre><code class="yml">https://weibo.cn/{uid}/profile?filter={filter_type}&amp;page={page_num}

# example
https://weibo.cn/2657006573/profile?filter=0&amp;page=1
uid: 2657006573
filter_type: 0
page_num: 1
</code></pre>
<p>注意，是<code>weibo.cn</code>而不是<code>weibo.com</code>，至于我是怎么找到这个页面的，说实话，我也忘了。。。</p>
<p>链接中包含3个参数，<code>uid</code>, <code>filter_mode</code> 以及 <code>page_num</code>。其中，<code>uid</code>就是前面提及的用户ID，<code>page_num</code>也很好理解，就是分页的当前页数，从1开始增加，那么，这个<code>filter_mode</code>是什么呢？</p>
<p>不着急，我们先来看看页面↓</p>
<p><img src="/assets/spider/sina/filter_mode.png" alt="filter mode of pictures"></p>
<p>可以看到，滤波类型<code>filter_mode</code>指的就是筛选条件，一共三个：</p>
<ol>
<li>filter=0 全部微博（包含纯文本微博，转载微博）</li>
<li>filter=1 原创微博（包含纯文本微博）</li>
<li>filter=2 图片微博（必须含有图片，包含转载）</li>
</ol>
<p>我通常会选择<strong>原创</strong>，因为我并不希望爬取结果中包含转载微博中的图片。当然，大家依照自己的需要选择即可。</p>
<h3 id="图链解析"><a href="#图链解析" class="headerlink" title="图链解析"></a>图链解析</h3><p>好了，参数来源都知道了，我们回过头看看这个网页。页面是不是感觉就是个空架子？毫无css痕迹，没关系，新浪本来就没打算把这个页面主动呈现给用户。但对于爬虫而言，这却是极好的，为什么这么说？原因如下：</p>
<ol>
<li>图片齐全，没有遗漏，就是个可视化的数据库</li>
<li>样式少，页面简单，省流量，爬取快</li>
<li>静态网页，分页存储，所见即所得</li>
<li>源码包含了所有微博的<strong>首图</strong>和<strong>组图链接</strong></li>
</ol>
<p>这样的网页用来练手再合适不过。但要注意的是上面第4点，什么是<strong>首图</strong>和<strong>组图链接</strong>呢，很好理解。每篇博客可能包含多张图片，那就是<strong>组图</strong>，但该页面只显示博客的第一张图片，即所谓的<strong>首图</strong>，<strong>组图链接</strong>指向的是存储着该组图所有图片的网址。</p>
<p>由于本人微博没组图，所以此处以刘亦菲微博为例，说明单图及组图的图链格式</p>
<p><img src="/assets/spider/sina/pictures.png" alt="pictures"></p>
<p>图中的上面一篇微博只有一张图片，可以轻易获取到原图链接，注意是<strong>原图</strong>，因为我们在页面能看到的是缩略图，但要爬取的当然是<strong>原图</strong>啦。</p>
<p>图中下面的微博包含组图，在图片右侧的<code>Chrome</code>开发工具可以看到组图链接。</p>
<p><a href="https://weibo.cn/mblog/picAll/FCQefgeAr?rl=2" target="_blank" rel="noopener">https://weibo.cn/mblog/picAll/FCQefgeAr?rl=2</a></p>
<p>打开组图链接，可以看到图片如下图所示：</p>
<p><img src="/assets/spider/sina/picture_url.png" alt="picture&#39;s url"></p>
<p>可以看到缩略图链接以及原图链接，然后我们点击<strong>原图</strong>看一下。</p>
<p><img src="/assets/spider/sina/picture_source.png" alt="picture&#39;s origin url"></p>
<p>可以发现，弹出页面的链接与上图显示的不同，但与上图中的缩略图链接极为相似。它们分别是：</p>
<ol>
<li>缩略图：<a href="http://ww1.sinaimg.cn/thumb180/c260f7ably1fn4vd7ix0qj20rs1aj1kx.jpg" target="_blank" rel="noopener">http://ww1.sinaimg.cn/thumb180/c260f7ably1fn4vd7ix0qj20rs1aj1kx.jpg</a></li>
<li>原图：<a href="http://wx1.sinaimg.cn/large/c260f7ably1fn4vd7ix0qj20rs1aj1kx.jpg" target="_blank" rel="noopener">http://wx1.sinaimg.cn/large/c260f7ably1fn4vd7ix0qj20rs1aj1kx.jpg</a></li>
</ol>
<p>可以看出，只是一个<code>thumb180</code>和<code>large</code>的区别。既然发现了规律，那就好办多了，我们只要知道缩略图的网址，就可以将域名后的第一级子域名替换成<code>large</code>就可以了，而不用获取<strong>原图</strong>链接再跳转一次。</p>
<p>而且，多次尝试可以发现组图链接及缩略图链接满足正则表达式：</p>
<pre><code class="python"># 1. 组图链接：
imglist_reg = r&#39;href=&quot;(https://weibo.cn/mblog/picAll/.{9}\?rl=2)&quot;&#39;

# 2. 缩略图
img_reg = r&#39;src=&quot;(http://w.{2}\.sinaimg.cn/(.{6,8})/.{32,33}.(jpg|gif))&quot;&#39;
</code></pre>
<p>到此，新浪微博的解析过程就结束了，图链的格式以及获取方式也都清楚了。下面就可以设计方案进行爬取了。</p>
<h2 id="确定爬取方案"><a href="#确定爬取方案" class="headerlink" title="确定爬取方案"></a>确定爬取方案</h2><p>根据解析结果，很容易制定出以下爬取方案：</p>
<ol>
<li>给定微博用户名<code>litreily</code></li>
<li>进入待爬取用户主页，即可从网址中获取<code>uid: 2657006573</code></li>
<li>获取本人登录微博后的<code>cookies</code>（请求报文需要用到<code>cookies</code>）</li>
<li>逐一爬取<a href="https://weibo.cn/2657006573/profile?filter=0&amp;page={1,2,3,...}" target="_blank" rel="noopener">https://weibo.cn/2657006573/profile?filter=0&amp;page={1,2,3,...}</a></li>
<li>解析每一页的源码，获取单图链接及组图链接，<ul>
<li>单图：直接获取该图缩略图链接；</li>
<li>组图：爬取组图链接，循环获取组图页面所有图片的缩略图链接</li>
</ul>
</li>
<li>循环将第5步获取到的图链替换为原图链接，并下载至本地</li>
<li>重复第4-6步，直至没有图片</li>
</ol>
<h3 id="获取cookies"><a href="#获取cookies" class="headerlink" title="获取cookies"></a>获取cookies</h3><p>针对以上方案，其中有几个重点内容，其一就是<code>cookies</code>的获取，我暂时还没学怎么自动获取<code>cookies</code>，所以目前是登录微博后手动获取的。</p>
<p><img src="/assets/spider/sina/cookies.png" alt="get cookies"></p>
<h3 id="下载网页"><a href="#下载网页" class="headerlink" title="下载网页"></a>下载网页</h3><p>下载网页用的是<code>python3</code>自带的<code>urllib</code>库，当时没学<code>requests</code>，以后可能也很少用<code>urllib</code>了。</p>
<pre><code class="python">def _get_html(url, headers):
    try:
        req = urllib.request.Request(url, headers = headers)
        page = urllib.request.urlopen(req)
        html = page.read().decode(&#39;UTF-8&#39;)
    except Exception as e:
        print(&quot;get %s failed&quot; % url)
        return None
    return html
</code></pre>
<h3 id="获取存储路径"><a href="#获取存储路径" class="headerlink" title="获取存储路径"></a>获取存储路径</h3><p>由于我是在<code>win10</code>下编写的代码，但是个人比较喜欢用<code>bash</code>，所以图片的存储路径有以下两种格式，<code>_get_path</code>函数会自动判断当前操作系统的类型，然后选择相应的路径。</p>
<pre><code class="python">def _get_path(uid):
    path = {
        &#39;Windows&#39;: &#39;D:/litreily/Pictures/python/sina/&#39; + uid,
        &#39;Linux&#39;: &#39;/mnt/d/litreily/Pictures/python/sina/&#39; + uid
    }.get(platform.system())

    if not os.path.isdir(path):
        os.makedirs(path)
    return path
</code></pre>
<p>幸好<code>windows</code>是兼容<code>linux</code>系统的斜杠符号的，不然程序中的相对路径替换还挺麻烦。</p>
<h3 id="下载图片"><a href="#下载图片" class="headerlink" title="下载图片"></a>下载图片</h3><p>由于选用的<code>urllib</code>库，所以下载图片就使用<code>urllib.request.urlretrieve</code>了</p>
<pre><code class="python"># image url of one page is saved in imgurls
for img in imgurls:
    imgurl = img[0].replace(img[1], &#39;large&#39;)
    num_imgs += 1
    try:
        urllib.request.urlretrieve(imgurl, &#39;{}/{}.{}&#39;.format(path, num_imgs, img[2]))
        # display the raw url of images
        print(&#39;\t%d\t%s&#39; % (num_imgs, imgurl))
    except Exception as e:
        print(str(e))
        print(&#39;\t%d\t%s failed&#39; % (num_imgs, imgurl))
</code></pre>
<h2 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h2><p>其它细节详见源码</p>
<pre><code class="python">#!/usr/bin/python3
# -*- coding:utf-8 -*-
# author: litreily
# date: 2018.02.05
&quot;&quot;&quot;Capture pictures from sina-weibo with user_id.&quot;&quot;&quot;

import re
import os
import platform

import urllib
import urllib.request

from bs4 import BeautifulSoup


def _get_path(uid):
    path = {
        &#39;Windows&#39;: &#39;D:/litreily/Pictures/python/sina/&#39; + uid,
        &#39;Linux&#39;: &#39;/mnt/d/litreily/Pictures/python/sina/&#39; + uid
    }.get(platform.system())

    if not os.path.isdir(path):
        os.makedirs(path)
    return path


def _get_html(url, headers):
    try:
        req = urllib.request.Request(url, headers = headers)
        page = urllib.request.urlopen(req)
        html = page.read().decode(&#39;UTF-8&#39;)
    except Exception as e:
        print(&quot;get %s failed&quot; % url)
        return None
    return html


def _capture_images(uid, headers, path):
    filter_mode = 1      # 0-all 1-original 2-pictures
    num_pages = 1
    num_blogs = 0
    num_imgs = 0

    # regular expression of imgList and img
    imglist_reg = r&#39;href=&quot;(https://weibo.cn/mblog/picAll/.{9}\?rl=2)&quot;&#39;
    imglist_pattern = re.compile(imglist_reg)
    img_reg = r&#39;src=&quot;(http://w.{2}\.sinaimg.cn/(.{6,8})/.{32,33}.(jpg|gif))&quot;&#39;
    img_pattern = re.compile(img_reg)

    print(&#39;start capture picture of uid:&#39; + uid)
    while True:
        url = &#39;https://weibo.cn/%s/profile?filter=%s&amp;page=%d&#39; % (uid, filter_mode, num_pages)

        # 1. get html of each page url
        html = _get_html(url, headers)

        # 2. parse the html and find all the imgList Url of each page
        soup = BeautifulSoup(html, &quot;lxml&quot;)
        # &lt;div class=&quot;c&quot; id=&quot;M_G4gb5pY8t&quot;&gt;&lt;div&gt;
        blogs = soup.body.find_all(attrs={&#39;id&#39;:re.compile(r&#39;^M_&#39;)}, recursive=False)
        num_blogs += len(blogs)

        imgurls = []
        for blog in blogs:
            blog = str(blog)
            imglist_url = imglist_pattern.findall(blog)
            if not imglist_url:
                # 2.1 get img-url from blog that have only one pic
                imgurls += img_pattern.findall(blog)
            else:
                # 2.2 get img-urls from blog that have group pics
                html = _get_html(imglist_url[0], headers)
                imgurls += img_pattern.findall(html)

        if not imgurls:
            print(&#39;capture complete!&#39;)
            print(&#39;captured pages:%d, blogs:%d, imgs:%d&#39; % (num_pages, num_blogs, num_imgs))
            print(&#39;directory:&#39; + path)
            break

        # 3. download all the imgs from each imgList
        print(&#39;PAGE %d with %d images&#39; % (num_pages, len(imgurls)))
        for img in imgurls:
            imgurl = img[0].replace(img[1], &#39;large&#39;)
            num_imgs += 1
            try:
                urllib.request.urlretrieve(imgurl, &#39;{}/{}.{}&#39;.format(path, num_imgs, img[2]))
                # display the raw url of images
                print(&#39;\t%d\t%s&#39; % (num_imgs, imgurl))
            except Exception as e:
                print(str(e))
                print(&#39;\t%d\t%s failed&#39; % (num_imgs, imgurl))
        num_pages += 1
        print(&#39;&#39;)


def main():
    # uids = [&#39;2657006573&#39;,&#39;2173752092&#39;,&#39;3261134763&#39;,&#39;2174219060&#39;]
    uid = &#39;2657006573&#39;
    path = _get_path(uid)

    # cookie is form the above url-&gt;network-&gt;request headers
    cookies = &#39;&#39;
    headers = {&#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36&#39;,
            &#39;Cookie&#39;: cookies}

    # capture imgs from sina
    _capture_images(uid, headers, path)


if __name__ == &#39;__main__&#39;:
    main()

</code></pre>
<p>使用时记得修改<code>main</code>函数中的<code>cookies</code>和<code>uid</code>！</p>
<h2 id="爬取测试"><a href="#爬取测试" class="headerlink" title="爬取测试"></a>爬取测试</h2><p><img src="/assets/spider/sina/capturer_litreily.png" alt="capture litreily"></p>
<p><img src="/assets/spider/sina/capturer_litreily_end.png" alt="capture litreily end"></p>
<p><img src="/assets/spider/sina/captured_pictures.png" alt="captured pictures"></p>
<h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><ul>
<li>该爬虫已存放至开源项目<a href="https://github.com/Litreily/capturer" target="_blank" rel="noopener">capturer</a>，欢迎交流</li>
<li>由于是首个爬虫，所以许多地方有待改进，相对的<a href="http://www.litreily.top/2018/03/17/lofter/">LOFTER爬虫</a>就更娴熟写了</li>
<li>目前没有发现新浪微博有明显的反爬措施，但还是按需索取为好</li>
</ul>
</div><div class="p-copyright"><blockquote><div class="p-copyright-author"><span class="p-copyright-key">本文作者：</span><span class="p-copytight-value"><a href="mailto:litreily@163.com">litreily</a></span></div><div class="p-copyright-link"><span class="p-copyright-key">本文链接：</span><span class="p-copytight-value"><a href="/2018/04/10/sina/">https://www.litreily.top/2018/04/10/sina/</a></span></div><div class="p-copyright-note"><span class="p-copyright-key">版权声明：</span><span class="p-copytight-value">本博客所有文章除特殊声明外，均采用<a rel="nofollow" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/"> CC BY-NC 4.0 </a>许可协议。转载请注明出处 <a href="https://www.litreily.top">litreily的博客</a>！</span></div></blockquote></div></article><div class="p-info box"><span class="p-tags"><i class="fa fa-tags"></i><a href="/tags/tools/">tools</a><a href="/tags/spider/">spider</a><a href="/tags/sina/">sina</a></span></div><aside id="toc"><div class="toc-title">目录</div><nav><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#分析sina站点"><span class="toc-number">1.</span> <span class="toc-text">分析sina站点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#获取用户ID"><span class="toc-number">1.1.</span> <span class="toc-text">获取用户ID</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#图片存储参数解析"><span class="toc-number">1.2.</span> <span class="toc-text">图片存储参数解析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#图链解析"><span class="toc-number">1.3.</span> <span class="toc-text">图链解析</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#确定爬取方案"><span class="toc-number">2.</span> <span class="toc-text">确定爬取方案</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#获取cookies"><span class="toc-number">2.1.</span> <span class="toc-text">获取cookies</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#下载网页"><span class="toc-number">2.2.</span> <span class="toc-text">下载网页</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#获取存储路径"><span class="toc-number">2.3.</span> <span class="toc-text">获取存储路径</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#下载图片"><span class="toc-number">2.4.</span> <span class="toc-text">下载图片</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#源码"><span class="toc-number">3.</span> <span class="toc-text">源码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#爬取测试"><span class="toc-number">4.</span> <span class="toc-text">爬取测试</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#写在最后"><span class="toc-number">5.</span> <span class="toc-text">写在最后</span></a></li></ol></nav></aside></div><section class="p-ext"><div class="l-pager l-pager-dtl box"><a class="prev" href="/2018/04/30/cfachina/">&lt; Python网络爬虫3 - 生产者消费者模型爬取某金融网站数据</a><a class="next" href="/2018/03/17/lofter/">Python网络爬虫1 - 爬取网易LOFTER图片 &gt;</a></div><div id="valine-comment"><style type="text/css">.night .v[data-class=v] a { color: #0F9FB4 !important; }
.night .v[data-class=v] a:hover { color: #216C73 !important; }
.night .v[data-class=v] li { list-style: inherit; }
.night .v[data-class=v] .vwrap { border: 1px solid #223441; border-radius: 0; }
.night .v[data-class=v] .vwrap:hover { box-shadow: 0 0 6px 1px #223441; }
.night .v[data-class=v] .vbtn { border-radius: 0; background: none; }
.night .v[data-class=v] .vlist .vcard .vh { border-bottom-color: #293D4E; }
.night .v[data-class=v] .vwrap .vheader .vinput { border-bottom-color: #223441; }
.night .v[data-class=v] .vwrap .vheader .vinput:focus { border-bottom-color: #339EB4; }
.night .v[data-class=v] code, .night .v[data-class=v] pre,.night .v[data-class=v] .vlist .vcard .vhead .vsys { background: #203240 !important; }
.night .v[data-class=v] code, .night .v[data-class=v] pre { color: #F0F0F0; font-size: 95%; }
.v[data-class=v] .vcards .vcard .vh {border-bottom-color: #223441; }
.night .v[data-class=v] .vcards .vcard .vcontent.expand:before {background: linear-gradient(180deg,rgba(38,57,73,.4),rgba(38,57,73,.9));}
.night .v[data-class=v] .vcards .vcard .vcontent.expand:after {background: rgba(38,57,73,.9)}
</style><div id="vcomment"></div><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'1ecKy4yk4u1R7C4tScKbnyq9-gzGzoHsz',
  appKey:'uvA3xgqNW3q8TGR483lxXcpB',
  lang: 'zh-cn',
  placeholder:'ヾﾉ≧∀≦)o Come on, say something...',
  avatar:'identicon',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></section><footer><p>Copyright © 2016 - 2020 <a href="/." rel="nofollow">LITREILY</a> | <strong><a rel="nofollow" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a></strong><br><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span></span> <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span></span> | Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a>Theme with<a rel="nofollow" target="_blank" href="https://github.com/litreily/snark-hexo"> snark.</a></p></footer></div></div></div><script type="text/javascript" src="/plugins/prettify/prettify.js"></script><script type="text/javascript" src="/js/search.js"></script><script type="text/javascript" src="/js/top.js"></script><script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
    search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.1" async></script></body></html>