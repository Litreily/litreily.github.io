<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="simple life"><meta name="theme-color" content="#2d4356"><meta name="baidu-site-verification" content="pte8o83UGG"><title>Python网络爬虫3 - 生产者消费者模型爬取某金融网站数据 | LITREILY</title><link rel="stylesheet" type="text/css" href="/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.png"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script><script>var _hmt = _hmt || [];
(function() {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?d55250b3059d32736607d30baa6e0ca2";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
})();</script></head><link rel="stylesheet" type="text/css" href="/plugins/prettify/doxy.css"><script type="text/javascript" src="/js/ready.js" async></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><body class="night"><div class="mobile-head" id="mobile-head"><div class="navbar-icon"><span></span><span></span><span></span></div><div class="navbar-title"><a href="/">LITREILY</a></div><div class="navbar-search"><!--= show a circle here--></div></div><div class="h-wrapper" id="menu"><nav class="h-head box"><div class="m-hdimg"><a class="hdimg img" href="/"><img class="nofancybox" src="/img/profile.jpg" width="128" height="128"></a><h1 class="ttl"><a href="/">LITREILY</a></h1></div><p class="m-desc">心之所向，无惧无悔,<br>愿求仁得仁，复无怨怼！</p><div class="m-nav"><ul><li><span class="dot">●</span><a href="/archives/">归档</a></li><li><span class="dot">●</span><a href="/categories/">分类</a></li><li><span class="dot">●</span><a href="/tags/">标签</a></li><li><span class="dot">●</span><a href="/about/">关于</a></li><li><span class="dot">●</span><a href="/notes/">笔记</a></li><li><span class="dot">●</span><a href="/atom.xml">RSS</a></li><li class="m-sch"><form class="form" id="j-formsch" method="get"><input class="txt" type="text" id="local-search-input" name="q" value="搜索" onfocus="if(this.value=='搜索'){this.value='';}" onblur="if(this.value==''){this.value='搜索';}"><input type="text" style="display:none;"></form></li></ul><div id="local-search-result"></div></div></nav></div><div id="back2Top"><a class="fa fa-arrow-up" title="Back to top" href="#"></a></div><div class="box" id="container"><div class="l-wrapper"><div class="l-content box"><div class="l-post l-post-art"><article class="p-art"><div class="p-header box"><h1 class="p-title">Python网络爬虫3 - 生产者消费者模型爬取某金融网站数据</h1><div class="p-info"><span class="p-date"><i class="fa fa-calendar"></i><a href="/2018/04/30/cfachina/">2018-04-30</a></span><span class="p-category"><i class="fa fa-folder"></i><a href="/categories/Python/">Python</a></span><span class="p-view" id="busuanzi_container_page_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_page_pv"></span></span></div></div><div class="p-content"><p>应一位金融圈的朋友所托，帮忙写个爬虫，帮他爬取<a href="http://www.cfachina.org/cfainfo/organbaseinfoServlet?all=personinfo#" target="_blank" rel="noopener">中国期货行业协议</a>网站中所有金融机构的从业人员信息。网站数据的获取本身比较简单，但是为了学习一些新的爬虫方法和技巧，即本文要讲述的<strong>生产者消费者模型</strong>，我又学习了一下Python中队列库<code>queue</code>及线程库<code>Thread</code>的使用方法。</p>
<h2 id="生产者消费者模型"><a href="#生产者消费者模型" class="headerlink" title="生产者消费者模型"></a>生产者消费者模型</h2><p>生产者消费者模型非常简单，相信大部分程序员都知道，就是一方作为生产者不断提供资源，另一方作为消费者不断消费资源。简单点说，就好比餐馆的厨师和顾客，厨师作为生产者不断制作美味的食物，而顾客作为消费者不断食用厨师提供的食物。此外，生产者与消费者之间可以是一对一、一对多、多对一和多对多的关系。</p>
<p>那么这个模型和爬虫有什么关系呢？其实，爬虫可以认为是一个生产者，它不断从网站爬取数据，爬取到的数据就是食物；而所得数据需要消费者进行数据清洗，把有用的数据吸收掉，把无用的数据丢弃。</p>
<p>在实践过程中，爬虫爬取和数据清洗分别对应一个<code>Thread</code>，两个线程之间通过顺序队列<code>queue</code>传递数据，数据传递过程就好比餐馆服务员从厨房把食物送到顾客餐桌上的过程。爬取线程负责爬取网站数据，并将原始数据存入队列，清洗线程从队列中按入队顺序读取原始数据并提取出有效数据。</p>
<p>以上便是对生产者消费者模型的简单介绍了，下面针对本次爬取任务予以详细说明。</p>
<h2 id="分析站点"><a href="#分析站点" class="headerlink" title="分析站点"></a>分析站点</h2><blockquote>
<p><a href="http://www.cfachina.org/cfainfo/organbaseinfoServlet?all=personinfo" target="_blank" rel="noopener">http://www.cfachina.org/cfainfo/organbaseinfoServlet?all=personinfo</a></p>
</blockquote>
<p><img src="/assets/spider/cfachina/home_page.png" alt="home page"></p>
<p>我们要爬取的数据是主页显示的表格中所有期货公司的<strong>从业人员信息</strong>，每个公司对应一个<strong>机构编号</strong>(<code>G01001~G01198</code>)。从上图可以看到有主页有分页，共8页。以<code>G01001</code>方正中期期货公司为例，点击该公司名称跳转至对应网页如下:</p>
<p><img src="/assets/spider/cfachina/personinfo.png" alt="personinfo"></p>
<p>从网址及网页内容可以提取出以下信息：</p>
<ol>
<li>网址<ul>
<li><a href="http://www.cfachina.org/cfainfo/organbaseinfoOneServlet?organid=+G01001+&amp;currentPage=1&amp;pageSize=20&amp;selectType=personinfo" target="_blank" rel="noopener">http://www.cfachina.org/cfainfo/organbaseinfoOneServlet?organid=+G01001+&amp;currentPage=1&amp;pageSize=20&amp;selectType=personinfo</a><ul>
<li><code>organid</code>: 机构编号，<code>+G01001+</code> ~ <code>+G01198+</code></li>
<li><code>currentPage</code>: 该机构从业人员信息当前页面编号</li>
<li><code>pageSize</code>: 每个页面显示的人员个数，默认20</li>
<li><code>selectType</code>: 固定为<code>personinfo</code></li>
</ul>
</li>
</ul>
</li>
<li>机构名称<code>mechanism_name</code>，在每页表格上方可以看到当前机构名称</li>
<li>从业人员信息，即每页的表格内容，也是我们要爬取的对象</li>
<li>该机构从业人员信息总页数<code>page_cnt</code></li>
</ol>
<p>我们最终爬取的数据可以按机构名称存储到对应的txt文件或excel文件中。</p>
<h3 id="获取机构名称"><a href="#获取机构名称" class="headerlink" title="获取机构名称"></a>获取机构名称</h3><p><img src="/assets/spider/cfachina/gst_title.png" alt="get mechanism name"></p>
<p>获取到某机构的任意从业信息页面后，使用<code>BeautifulSoup</code>可快速提取机构名称。</p>
<pre><code class="python">mechanism_name = soup.find(&#39;&#39;, {&#39;class&#39;:&#39;gst_title&#39;}).find_all(&#39;a&#39;)[2].get_text()
</code></pre>
<p>那么有人可能会问，既然主页表格都已经包含了所有机构的编号和名称，为何还要多此一举的再获取一次呢？这是因为，我压根就不想爬主页的那些表格，直接根据机构编号的递增规律生成对应的网址即可，所以获取机构名称的任务就放在了爬取每个机构首个信息页面之后。</p>
<h3 id="获取机构信息对应的网页数量"><a href="#获取机构信息对应的网页数量" class="headerlink" title="获取机构信息对应的网页数量"></a>获取机构信息对应的网页数量</h3><p><img src="/assets/spider/cfachina/page_cnt.png" alt="get count of page"></p>
<p>每个机构的数据量是不等的，幸好每个页面都包含了当前页面数及总页面数。使用以下代码即可获取页码数。</p>
<pre><code class="python">url_re = re.compile(&#39;#currentPage.*\+.*\+\&#39;(\d+)\&#39;&#39;)
page_cnt = url_re.search(html).group(1)
</code></pre>
<p>从每个机构首页获取页码数后，便可<code>for</code>循环修改网址参数中的<code>currentPage</code>，逐页获取机构信息。</p>
<h3 id="获取当前页面从业人员信息"><a href="#获取当前页面从业人员信息" class="headerlink" title="获取当前页面从业人员信息"></a>获取当前页面从业人员信息</h3><p><img src="/assets/spider/cfachina/personinfo_table.png" alt="get personinfo"></p>
<p>针对如上图所示的一个特定信息页时，人员信息被存放于一个表中，除了固定的表头信息外，人员信息均被包含在一个带有<code>id</code>的<code>tr</code>标签中，所以使用<code>BeautifulSoup</code>可以很容易提取出页面内所有人员信息。</p>
<pre><code class="python">soup.find_all(&#39;tr&#39;, id=True)
</code></pre>
<h2 id="确定爬取方案"><a href="#确定爬取方案" class="headerlink" title="确定爬取方案"></a>确定爬取方案</h2><p>一般的想法当然是逐页爬取主页信息，然后获取每页所有机构对应的网页链接，进而继续爬取每个机构信息。</p>
<p>但是由于该网站的机构信息网址具有明显的规律，我们根据每个机构的编号便可直接得到每个机构每个信息页面的网址。所以具体爬取方案如下：</p>
<ol>
<li>将所有<strong>机构编号</strong>网址存入队列<code>url_queue</code></li>
<li>新建生产者线程<code>SpiderThread</code>完成抓取任务<ul>
<li>循环从队列<code>url_queue</code>中读取一个编号，生成机构首页网址，使用<code>requests</code>抓取之</li>
<li>从抓取结果中获取页码数量，若为0，则返回该线程第1步</li>
<li>循环爬取当前机构剩余页面</li>
<li>将页面信息存入队列<code>html_queue</code></li>
</ul>
</li>
<li>新建消费者线程<code>DatamineThread</code>完成数据清洗任务<ul>
<li>循环从队列<code>html_queue</code>中读取一组页面信息</li>
<li>使用<code>BeautifulSoup</code>提取页面中的从业人员信息</li>
<li>将信息以二维数组形式存储，最后交由数据存储类<code>Storage</code>存入本地文件</li>
</ul>
</li>
</ol>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><h3 id="生成者SpiderThread"><a href="#生成者SpiderThread" class="headerlink" title="生成者SpiderThread"></a>生成者<code>SpiderThread</code></h3><p>爬虫线程先从队列获取一个机构编号，生成机构首页网址并进行爬取，接着判断机构页面数量是否为0，如若不为0则继续获取机构名称，并根据页面数循环爬取剩余页面，将原始html数据以如下<code>dict</code>格式存入队列<code>html_queue</code>:</p>
<pre><code class="json">{
    &#39;name&#39;: mechanismId_mechanismName,
    &#39;num&#39;: currentPage,
    &#39;content&#39;: html
}
</code></pre>
<p>爬虫产生的数据队列<code>html_queue</code>将由数据清洗线程进行处理，下面是爬虫线程的主程序，整个线程代码请看后面的<a href="#源码">源码</a>。</p>
<pre><code class="python">def run(self):
    while True:
        mechanism_id = &#39;G0&#39; + self.url_queue.get()

        # the first page&#39;s url
        url = self.__get_url(mechanism_id, 1)
        html = self.grab(url)

        page_cnt = self.url_re.search(html.text).group(1)
        if page_cnt == &#39;0&#39;:
            self.url_queue.task_done()
            continue

        soup = BeautifulSoup(html.text, &#39;html.parser&#39;)
        mechanism_name = soup.find(&#39;&#39;, {&#39;class&#39;:&#39;gst_title&#39;}).find_all(&#39;a&#39;)[2].get_text()
        print(&#39;\nGrab Thread: get %s - %s with %s pages\n&#39; % (mechanism_id, mechanism_name, page_cnt))

        # put data into html_queue
        self.html_queue.put({&#39;name&#39;:&#39;%s_%s&#39; % (mechanism_id, mechanism_name), &#39;num&#39;:1, &#39;content&#39;:html})
        for i in range(2, int(page_cnt) + 1):
            url = self.__get_url(mechanism_id, i)
            html = self.grab(url)
            self.html_queue.put({&#39;name&#39;:&#39;%s_%s&#39; % (mechanism_id, mechanism_name), &#39;num&#39;:i, &#39;content&#39;:html})

        self.url_queue.task_done()
</code></pre>
<h3 id="消费者DatamineThread"><a href="#消费者DatamineThread" class="headerlink" title="消费者DatamineThread"></a>消费者<code>DatamineThread</code></h3><p>数据清洗线程比较简单，就是从生产者提供的数据队列<code>html_queue</code>逐一提取<code>html</code>数据，然后从<code>html</code>数据中提取从业人员信息，以二维数组形式存储，最后交由存储模块<code>Storage</code>完成数据存储工作。</p>
<pre><code class="python">class DatamineThread(Thread):
    &quot;&quot;&quot;Parse data from html&quot;&quot;&quot;
    def __init__(self, html_queue, filetype):
        Thread.__init__(self)
        self.html_queue = html_queue
        self.filetype = filetype

    def __datamine(self, data):
        &#39;&#39;&#39;Get data from html content&#39;&#39;&#39;
        soup = BeautifulSoup(data[&#39;content&#39;].text, &#39;html.parser&#39;)
        infos = []
        for info in soup.find_all(&#39;tr&#39;, id=True):
            items = []
            for item in info.find_all(&#39;td&#39;):
                items.append(item.get_text())
            infos.append(items)
        return infos

    def run(self):
        while True:
            data = self.html_queue.get()
            print(&#39;Datamine Thread: get %s_%d&#39; % (data[&#39;name&#39;], data[&#39;num&#39;]))

            store = Storage(data[&#39;name&#39;], self.filetype)
            store.save(self.__datamine(data))
            self.html_queue.task_done()
</code></pre>
<h3 id="数据存储Storage"><a href="#数据存储Storage" class="headerlink" title="数据存储Storage"></a>数据存储<code>Storage</code></h3><p>我写了两类文件格式的存储函数，<code>write_txt</code>, <code>write_excel</code>，分别对应<code>txt</code>,<code>excel</code>文件。实际存储时由调用方确定文件格式。</p>
<pre><code class="python">def save(self, data):
    {
        &#39;.txt&#39;: self.write_txt,
        &#39;.xls&#39;: self.write_excel
    }.get(self.filetype)(data)
</code></pre>
<h4 id="存入txt文件"><a href="#存入txt文件" class="headerlink" title="存入txt文件"></a>存入txt文件</h4><p>存入<code>txt</code>文件是比较简单的，就是以附加(<code>a</code>)形式打开文件，写入数据，关闭文件。其中，文件名称由调用方提供。写入数据时，每个人员信息占用一行，以制表符<code>\t</code>分隔。</p>
<pre><code class="python">def write_txt(self, data):
    &#39;&#39;&#39;Write data to txt file&#39;&#39;&#39;
    fid = open(self.path, &#39;a&#39;, encoding=&#39;utf-8&#39;)

    # insert the header of table
    if not os.path.getsize(self.path):
        fid.write(&#39;\t&#39;.join(self.table_header) + &#39;\n&#39;)

    for info in data:
        fid.write(&#39;\t&#39;.join(info) + &#39;\n&#39;)
    fid.close()
</code></pre>
<h4 id="存入Excel文件"><a href="#存入Excel文件" class="headerlink" title="存入Excel文件"></a>存入Excel文件</h4><p>存入<code>Excel</code>文件还是比较繁琐的，由于经验不多，选用的是<code>xlwt</code>, <code>xlrd</code>和<code>xlutils</code>库。说实话，这3个库真心不大好用，勉强完成任务而已。为什么这么说，且看：</p>
<ol>
<li>修改文件麻烦：<code>xlwt</code>只能写,<code>xlrd</code>只能读，需要<code>xlutils</code>的<code>copy</code>函数将<code>xlrd</code>读取的数据复制到内存，再用<code>xlwt</code>修改</li>
<li>只支持<code>.xls</code>文件：<code>.xlsx</code>经读写也会变成<code>.xls</code>格式</li>
<li>表格样式易变：只要重新写入文件，表格样式必然重置</li>
</ol>
<p>所以后续我肯定会再学学其它的<code>excel</code>库，当然，当前解决方案暂时还用这三个。代码如下：</p>
<pre><code class="python">def write_excel(self, data):
    &#39;&#39;&#39;write data to excel file&#39;&#39;&#39;
    if not os.path.exists(self.path):
        header_style = xlwt.easyxf(&#39;font:name 楷体, color-index black, bold on&#39;)
        wb = xlwt.Workbook(encoding=&#39;utf-8&#39;)
        ws = wb.add_sheet(&#39;Data&#39;)

        # insert the header of table
        for i in range(len(self.table_header)):
            ws.write(0, i, self.table_header[i], header_style)
    else:
        rb = open_workbook(self.path)
        wb = copy(rb)
        ws = wb.get_sheet(0)

    # write data
    offset = len(ws.rows)
    for i in range(0, len(data)):
        for j in range(0, len(data[0])):
            ws.write(offset + i, j, data[i][j])

    # When use xlutils.copy.copy function to copy data from exist .xls file,
    # it will loss the origin style, so we need overwrite the width of column,
    # maybe there some other good solution, but I have not found yet.
    for i in range(len(self.table_header)):
        ws.col(i).width = 256 * (10, 10, 15, 20, 50, 20, 15)[i]

    # save to file
    while True:
        try:
            wb.save(self.path)
            break
        except PermissionError as e:
            print(&#39;{0} error: {1}&#39;.format(self.path, e.strerror))
            time.sleep(5)
        finally:
            pass
</code></pre>
<p>说明：</p>
<ol>
<li>一个文件对应一个机构的数据，需要多次读取和写入，所以需要计算文件写入时的行数偏移量<code>offset</code>，即当前文件已包含数据的行数；</li>
<li>当被写入文件被人为打开时，会出现<code>PermissionError</code>异常，可以在捕获该异常然后提示错误信息，并定时等待直到文件被关闭。</li>
</ol>
<h3 id="main"><a href="#main" class="headerlink" title="main"></a>main</h3><p>主函数用于创建和启动生产者线程和消费者线程，同时为生产者线程提供机构编号队列。</p>
<pre><code class="python">url_queue = queue.Queue()
html_queue = queue.Queue()

def main():
    for i in range(1001, 1199):
        url_queue.put(str(i))

    # create and start a spider thread
    st = SpiderThread(url_queue, html_queue)
    st.setDaemon(True)
    st.start()

    # create and start a datamine thread
    dt = DatamineThread(html_queue, &#39;.xls&#39;)
    dt.setDaemon(True)
    dt.start()

    # wait on the queue until everything has been processed
    url_queue.join()
    html_queue.join()
</code></pre>
<p>从主函数可以看到，两个队列都调用了<code>join</code>函数，用于阻塞，直到对应队列为空为止。要注意的是，队列操作中，<strong>每个出队操作<code>queue.get()</code>需要对应一个<code>queue.task_done()</code>操作</strong>，否则会出现队列数据已全部处理完，但主线程仍在执行的情况。</p>
<p>至此，爬虫的主要代码便讲解完了，下面是完整源码。</p>
<h2 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h2><pre><code class="python">#!/usr/bin/python3
# -*-coding:utf-8-*-

import queue
from threading import Thread

import requests

import re
from bs4 import BeautifulSoup

import os
import platform

import xlwt
from xlrd import open_workbook
from xlutils.copy import copy

import time

# url format ↓
# http://www.cfachina.org/cfainfo/organbaseinfoOneServlet?organid=+G01001+&amp;currentPage=1&amp;pageSize=20&amp;selectType=personinfo&amp;all=undefined
# organid: +G01001+, +G01002+, +G01003+, ...
# currentPage: 1, 2, 3, ...
# pageSize: 20(default)
#
# Algorithm design:
# 2 threads with 2 queues
# Thread-1, get first page url, then get page_num and mechanism_name from first page
# Thread-2, parse html file and get data from it, then output data to local file
# url_queue data -&gt; &#39;url&#39;  # first url of each mechanism
# html_queue data -&gt; {&#39;name&#39;:&#39;mechanism_name&#39;, &#39;html&#39;:data}

url_queue = queue.Queue()
html_queue = queue.Queue()


class SpiderThread(Thread):
    &quot;&quot;&quot;Threaded Url Grab&quot;&quot;&quot;
    def __init__(self, url_queue, html_queue):
        Thread.__init__(self)
        self.url_queue = url_queue
        self.html_queue = html_queue
        self.page_size = 20
        self.url_re = re.compile(&#39;#currentPage.*\+.*\+\&#39;(\d+)\&#39;&#39;)
        self.headers = {&#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36&#39;}

    def __get_url(self, mechanism_id, current_page):
        return &#39;http://www.cfachina.org/cfainfo/organbaseinfoOneServlet?organid=+%s+&amp;currentPage=%d&amp;pageSize=%d&amp;selectType=personinfo&amp;all=undefined&#39; \
        % (mechanism_id, current_page, self.page_size)

    def grab(self, url):
        &#39;&#39;&#39;Grab html of url from web&#39;&#39;&#39;
        while True:
            try:
                html = requests.get(url, headers=self.headers, timeout=20)
                if html.status_code == 200:
                    break
            except requests.exceptions.ConnectionError as e:
                print(url + &#39; Connection error, try again...&#39;)
            except requests.exceptions.ReadTimeout as e:
                print(url + &#39; Read timeout, try again...&#39;)
            except Exception as e:
                print(str(e))
            finally:
                pass
        return html

    def run(self):
        &#39;&#39;&#39;Grab all htmls of mechanism one by one
        Steps:
            1. grab first page of each mechanism from url_queue
            2. get number of pages and mechanism name from first page
            3. grab all html file of each mechanism
            4. push all html to html_queue
        &#39;&#39;&#39;
        while True:
            mechanism_id = &#39;G0&#39; + self.url_queue.get()

            # the first page&#39;s url
            url = self.__get_url(mechanism_id, 1)
            html = self.grab(url)

            page_cnt = self.url_re.search(html.text).group(1)
            if page_cnt == &#39;0&#39;:
                self.url_queue.task_done()
                continue

            soup = BeautifulSoup(html.text, &#39;html.parser&#39;)
            mechanism_name = soup.find(&#39;&#39;, {&#39;class&#39;:&#39;gst_title&#39;}).find_all(&#39;a&#39;)[2].get_text()
            print(&#39;\nGrab Thread: get %s - %s with %s pages\n&#39; % (mechanism_id, mechanism_name, page_cnt))

            # put data into html_queue
            self.html_queue.put({&#39;name&#39;:&#39;%s_%s&#39; % (mechanism_id, mechanism_name), &#39;num&#39;:1, &#39;content&#39;:html})
            for i in range(2, int(page_cnt) + 1):
                url = self.__get_url(mechanism_id, i)
                html = self.grab(url)
                self.html_queue.put({&#39;name&#39;:&#39;%s_%s&#39; % (mechanism_id, mechanism_name), &#39;num&#39;:i, &#39;content&#39;:html})

            self.url_queue.task_done()


class DatamineThread(Thread):
    &quot;&quot;&quot;Parse data from html&quot;&quot;&quot;
    def __init__(self, html_queue, filetype):
        Thread.__init__(self)
        self.html_queue = html_queue
        self.filetype = filetype

    def __datamine(self, data):
        &#39;&#39;&#39;Get data from html content&#39;&#39;&#39;
        soup = BeautifulSoup(data[&#39;content&#39;].text, &#39;html.parser&#39;)
        infos = []
        for info in soup.find_all(&#39;tr&#39;, id=True):
            items = []
            for item in info.find_all(&#39;td&#39;):
                items.append(item.get_text())
            infos.append(items)
        return infos

    def run(self):
        while True:
            data = self.html_queue.get()
            print(&#39;Datamine Thread: get %s_%d&#39; % (data[&#39;name&#39;], data[&#39;num&#39;]))

            store = Storage(data[&#39;name&#39;], self.filetype)
            store.save(self.__datamine(data))
            self.html_queue.task_done()


class Storage():
    def __init__(self, filename, filetype):
        self.filetype = filetype
        self.filename = filename + filetype
        self.table_header = (&#39;姓名&#39;, &#39;性别&#39;, &#39;从业资格号&#39;, &#39;投资咨询从业证书号&#39;, &#39;任职部门&#39;, &#39;职务&#39;, &#39;任现职时间&#39;)
        self.path = self.__get_path()

    def __get_path(self):
        path = {
            &#39;Windows&#39;: &#39;D:/litreily/Documents/python/cfachina&#39;,
            &#39;Linux&#39;: &#39;/mnt/d/litreily/Documents/python/cfachina&#39;
        }.get(platform.system())

        if not os.path.isdir(path):
            os.makedirs(path)
        return &#39;%s/%s&#39; % (path, self.filename)

    def write_txt(self, data):
        &#39;&#39;&#39;Write data to txt file&#39;&#39;&#39;
        fid = open(self.path, &#39;a&#39;, encoding=&#39;utf-8&#39;)

        # insert the header of table
        if not os.path.getsize(self.path):
            fid.write(&#39;\t&#39;.join(self.table_header) + &#39;\n&#39;)

        for info in data:
            fid.write(&#39;\t&#39;.join(info) + &#39;\n&#39;)
        fid.close()

    def write_excel(self, data):
        &#39;&#39;&#39;write data to excel file&#39;&#39;&#39;
        if not os.path.exists(self.path):
            header_style = xlwt.easyxf(&#39;font:name 楷体, color-index black, bold on&#39;)
            wb = xlwt.Workbook(encoding=&#39;utf-8&#39;)
            ws = wb.add_sheet(&#39;Data&#39;)

            # insert the header of table
            for i in range(len(self.table_header)):
                ws.write(0, i, self.table_header[i], header_style)
        else:
            rb = open_workbook(self.path)
            wb = copy(rb)
            ws = wb.get_sheet(0)

        # write data
        offset = len(ws.rows)
        for i in range(0, len(data)):
            for j in range(0, len(data[0])):
                ws.write(offset + i, j, data[i][j])

        # When use xlutils.copy.copy function to copy data from exist .xls file,
        # it will loss the origin style, so we need overwrite the width of column,
        # maybe there some other good solution, but I have not found yet.
        for i in range(len(self.table_header)):
            ws.col(i).width = 256 * (10, 10, 15, 20, 50, 20, 15)[i]

        # save to file
        while True:
            try:
                wb.save(self.path)
                break
            except PermissionError as e:
                print(&#39;{0} error: {1}&#39;.format(self.path, e.strerror))
                time.sleep(5)
            finally:
                pass

    def save(self, data):
        &#39;&#39;&#39;Write data to local file.

        According filetype to choose function to save data, filetype can be &#39;.txt&#39;
        or &#39;.xls&#39;, but &#39;.txt&#39; type is saved more faster then &#39;.xls&#39; type

        Args:
            data: a 2d-list array that need be save
        &#39;&#39;&#39;
        {
            &#39;.txt&#39;: self.write_txt,
            &#39;.xls&#39;: self.write_excel
        }.get(self.filetype)(data)


def main():
    for i in range(1001, 1199):
        url_queue.put(str(i))

    # create and start a spider thread
    st = SpiderThread(url_queue, html_queue)
    st.setDaemon(True)
    st.start()

    # create and start a datamine thread
    dt = DatamineThread(html_queue, &#39;.xls&#39;)
    dt.setDaemon(True)
    dt.start()

    # wait on the queue until everything has been processed
    url_queue.join()
    html_queue.join()


if __name__ == &#39;__main__&#39;:
    main()
</code></pre>
<h2 id="爬取测试"><a href="#爬取测试" class="headerlink" title="爬取测试"></a>爬取测试</h2><p><img src="/assets/spider/cfachina/spider.png" alt="spider"></p>
<p><img src="/assets/spider/cfachina/save_txt.png" alt="save to txt"></p>
<p><img src="/assets/spider/cfachina/save_xls.png" alt="save to excel"></p>
<h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><ul>
<li>测试发现，写入<code>txt</code>的速度明显高于写入<code>excel</code>的速度</li>
<li>如果将页面网址中的<code>pageSize</code>修改为<code>1000</code>或更大，则可以一次性获取某机构的所有从业人员信息，而不用逐页爬取，效率可以大大提高。</li>
<li>该爬虫已托管至<a href="https://github.com/Litreily/Python-demos" target="_blank" rel="noopener">github Python-demos</a></li>
</ul>
</div><div class="p-copyright"><blockquote><div class="p-copyright-author"><span class="p-copyright-key">本文作者：</span><span class="p-copytight-value"><a href="mailto:litreily@163.com">litreily</a></span></div><div class="p-copyright-link"><span class="p-copyright-key">本文链接：</span><span class="p-copytight-value"><a href="/2018/04/30/cfachina/">https://www.litreily.top/2018/04/30/cfachina/</a></span></div><div class="p-copyright-note"><span class="p-copyright-key">版权声明：</span><span class="p-copytight-value">本博客所有文章除特殊声明外，均采用<a rel="nofollow" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/"> CC BY-NC 4.0 </a>许可协议。转载请注明出处 <a href="https://www.litreily.top">litreily的博客</a>！</span></div></blockquote></div></article><div class="p-info box"><span class="p-tags"><i class="fa fa-tags"></i><a href="/tags/spider/">spider</a><a href="/tags/queue/">queue</a><a href="/tags/xlwt/">xlwt</a></span></div><aside id="toc"><div class="toc-title">目录</div><nav><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#生产者消费者模型"><span class="toc-number">1.</span> <span class="toc-text">生产者消费者模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#分析站点"><span class="toc-number">2.</span> <span class="toc-text">分析站点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#获取机构名称"><span class="toc-number">2.1.</span> <span class="toc-text">获取机构名称</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#获取机构信息对应的网页数量"><span class="toc-number">2.2.</span> <span class="toc-text">获取机构信息对应的网页数量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#获取当前页面从业人员信息"><span class="toc-number">2.3.</span> <span class="toc-text">获取当前页面从业人员信息</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#确定爬取方案"><span class="toc-number">3.</span> <span class="toc-text">确定爬取方案</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#代码实现"><span class="toc-number">4.</span> <span class="toc-text">代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#生成者SpiderThread"><span class="toc-number">4.1.</span> <span class="toc-text">生成者SpiderThread</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#消费者DatamineThread"><span class="toc-number">4.2.</span> <span class="toc-text">消费者DatamineThread</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据存储Storage"><span class="toc-number">4.3.</span> <span class="toc-text">数据存储Storage</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#存入txt文件"><span class="toc-number">4.3.1.</span> <span class="toc-text">存入txt文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#存入Excel文件"><span class="toc-number">4.3.2.</span> <span class="toc-text">存入Excel文件</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#main"><span class="toc-number">4.4.</span> <span class="toc-text">main</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#源码"><span class="toc-number">5.</span> <span class="toc-text">源码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#爬取测试"><span class="toc-number">6.</span> <span class="toc-text">爬取测试</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#写在最后"><span class="toc-number">7.</span> <span class="toc-text">写在最后</span></a></li></ol></nav></aside></div><section class="p-ext"><div class="l-pager l-pager-dtl box"><a class="prev" href="/2018/05/27/scrapy-start/">&lt; Python网络爬虫4 - scrapy入门</a><a class="next" href="/2018/04/10/sina/">Python网络爬虫2 - 爬取新浪微博用户图片 &gt;</a></div><div id="valine-comment"><style type="text/css">.night .v[data-class=v] a { color: #0F9FB4 !important; }
.night .v[data-class=v] a:hover { color: #216C73 !important; }
.night .v[data-class=v] li { list-style: inherit; }
.night .v[data-class=v] .vwrap { border: 1px solid #223441; border-radius: 0; }
.night .v[data-class=v] .vwrap:hover { box-shadow: 0 0 6px 1px #223441; }
.night .v[data-class=v] .vbtn { border-radius: 0; background: none; }
.night .v[data-class=v] .vlist .vcard .vh { border-bottom-color: #293D4E; }
.night .v[data-class=v] .vwrap .vheader .vinput { border-bottom-color: #223441; }
.night .v[data-class=v] .vwrap .vheader .vinput:focus { border-bottom-color: #339EB4; }
.night .v[data-class=v] code, .night .v[data-class=v] pre,.night .v[data-class=v] .vlist .vcard .vhead .vsys { background: #203240 !important; }
.night .v[data-class=v] code, .night .v[data-class=v] pre { color: #F0F0F0; font-size: 95%; }
.v[data-class=v] .vcards .vcard .vh {border-bottom-color: #223441; }
.night .v[data-class=v] .vcards .vcard .vcontent.expand:before {background: linear-gradient(180deg,rgba(38,57,73,.4),rgba(38,57,73,.9));}
.night .v[data-class=v] .vcards .vcard .vcontent.expand:after {background: rgba(38,57,73,.9)}
</style><div id="vcomment"></div><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'1ecKy4yk4u1R7C4tScKbnyq9-gzGzoHsz',
  appKey:'uvA3xgqNW3q8TGR483lxXcpB',
  lang: 'zh-cn',
  placeholder:'ヾﾉ≧∀≦)o Come on, say something...',
  avatar:'identicon',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></section><footer><p>Copyright © 2016 - 2020 <a href="/." rel="nofollow">LITREILY</a> | <strong><a rel="nofollow" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a></strong><br><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span></span> <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span></span> | Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a>Theme with<a rel="nofollow" target="_blank" href="https://github.com/litreily/snark-hexo"> snark.</a></p></footer></div></div></div><script type="text/javascript" src="/plugins/prettify/prettify.js"></script><script type="text/javascript" src="/js/search.js"></script><script type="text/javascript" src="/js/top.js"></script><script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
    search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.1" async></script></body></html>